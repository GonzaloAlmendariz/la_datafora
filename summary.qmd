# Estructura del libro {.unnumbered}

Este libro es para cualquier persona que desee tener una comprensión sólida de los principios y herramientas estadísticas que se utilizan habitualmente en el ámbito de la investigación, la administración pública y el mundo empresarial. Sin embargo, mi formación como politólogo me ha llevado inevitablemente a enfatizar los aspectos prácticos y empíricos, de modo que, aunque describo y explico conceptos matemáticos importantes, el libro está diseñado en última instancia para los científicos sociales que buscan trabajar con una base general y práctica para usar R en el análisis estadístico. Así que no es un libro muy profundo matemáticamente. Aquello no implica que no se haya hecho especial énfasis en que el lector pueda saber considerar herramientas pertinentes, interpretar sus resultados y reconocer las limitaciones que estos acarrean.

Este libro está organizado en nueve capítulos que abarcan un recorrido intencional (y espero ameno) desde los fundamentos de la estadística y la programación en R, hasta el uso de pruebas estadísticas, modelado y visualización de datos. Aunque la estructura ha sido concebida para que el lector pueda asimilar gradualmente los conceptos necesarios, también se ha procurado que cada capítulo conserve cierta autonomía. Así, el lector tiene la libertad de situarse en el punto del libro que le resulte más pertinente o familiar, siendo consciente, sin embargo, de que el nivel de dificultad incrementa de forma progresiva y que ciertos contenidos requerirán mayor dedicación al introducir nuevos conceptos. Sin más que añadir, procedo con la estructura del libro.

**Capítulo 1: Fundamentos en Estadística**

Este capítulo marca el inicio formal del recorrido. Se abordan conceptos centrales como la definición de datos, la naturaleza de las variables y la implicancia de la variabilidad, todos ellos fundamentales para comprender qué se mide y cómo. También se presenta una introducción sistemática a las técnicas de muestreo, con énfasis en sus modalidades más utilizadas. Finalmente, se introduce y explica la diferencias entre la estadística descriptiva y la estadística inferencial, estableciendo así el marco conceptual para todo lo que vendrá después.

**Capítulo 2: Fundamentos en R**

El lenguaje R será nuestra herramienta principal a lo largo del libro. En este capítulo se parte de cero: guiamos paso a paso por la instalación de R y RStudio, exploramos su interfaz y aprendemos a organizar un entorno de trabajo estructurado y reproducible. Se introducen conceptos esenciales como objetos, vectores, funciones, paquetes y operaciones básicas, todo explicado de forma accesible pero rigurosa. El enfoque es práctico: se asume que el lector no tiene experiencia previa programando, pero que sí posee el interés genuino por desarrollar habilidades reales para analizar datos con R.

**Capítulo 3: Integración**

Este capítulo representa el corazón operativo del libro. Aquí se empieza a trabajar con bases de datos reales en estructuras tipo data frame, se introduce el flujo de trabajo del Tidyverse y se consolidan los aprendizajes anteriores. Se busca que el lector aprenda a importar, limpiar, transformar y resumir datos utilizando herramientas como `readr`, `janitor`, `dplyr` y `tidyr`. También se aborda de forma introductoria la visualización con `ggplot2`, cubriendo los fundamentos de su gramática y también estrategias visuales, diseño de gráficos y mejores prácticas. Es en este capítulo donde las piezas comienzan a encajar: el lenguaje, los datos, las transformaciones y la visualización se integran de forma coherente.

**Capítulo 4: Estadística descriptiva**

Este capítulo se ocupa de todo lo relacionado con estadística descriptiva. Se desarrollan medidas de tendencia central (media, mediana, moda), dispersión (rango, varianza, desviación estándar), posición (cuartiles, percentiles) y forma de la distribución (asimetría, curtosis). También se examina el resumen de variables categóricas a través de tablas de frecuencia y proporciones. Se aplican estos conceptos en R, tanto con funciones específicas como mediante visualizaciones elaboradas con `ggplot2`. El capítulo culmina con una sección de análisis exploratorio univariado y bivariado, que permite al lector identificar patrones, inconsistencias y relaciones antes de avanzar hacia la inferencia.

**Capítulo 5: Probabilidad**

La probabilidad es la base de toda la inferencia estadística y aquí se busca generar una base sólida para poder comprender sus técnicas. Empezamos definiendo la probabilidad desde un enfoque frecuentista y poniendo en práctica los conceptos de la probabilidad teórica y la experimental. Proseguimos con las distribuciones de probabilidad y sus características. Se discuten distribuciones discretas y continuas (binomial, Poisson, normal), cómo trabajar con ellas en R. Luego, se hace mayor énfasis en la distribución normal, la regla empírica, cómo interpretar valores z y las áreas bajo la curva. Finalmente se introduce el teorema del límite central como puente para lo demás capítulos.

**Capítulo 6: Estadística inferencial**

A partir de aquí entramos en el terreno de las decisiones y generalizaciones. Este capítulo retoma el concepto del teorema del límite central y se concentra en su aplicación para la estimación por intervalos y el contraste de hipótesis. Se abordan también elementos clave como la significancia estadística, los valores *p*, los errores tipo I y II, y la validación de supuestos, todo ello acompañado de visualizaciones que buscan facilitar la comprensión e interpretación de los resultados. Se introduce la distribución *t* de Student y su uso en pruebas de hipótesis con muestras pequeñas. A lo largo del capítulo, se entrelazan los fundamentos teóricos con su aplicación en R, con el objetivo de que el lector pueda evaluar la validez de una afirmación estadística sin perder de vista la importancia del contexto.

**Capítulo 7: Comparando grupos**

Aquí se desarrollan técnicas para comparar grupos entre sí. Se estudian las diferencias entre medias y proporciones, tanto en muestras independientes como relacionadas. Se explica cómo aplicar pruebas t, ANOVA y pruebas de chi-cuadrado, abordando sus supuestos y condiciones de uso. También se introducen métodos no paramétricos para situaciones en las que los supuestos tradicionales no se cumplen. Finalmente, el lector es introducido al paquete `infer`, que permite hacer inferencia estadística mediante simulaciones.

**Capítulo 8: Regresión**

La regresión es una de las herramientas más populares y versátiles del análisis estadístico y no podía quedar fuera. Este capítulo comienza con la regresión lineal simple: se presenta la ecuación que la define, sus parámetros fundamentales, el proceso de estimación y las principales limitaciones del modelo. Se abordan conceptos clave como la desviación estándar condicional, la relación entre correlación y pendiente, el coeficiente de determinación y la interpretación práctica de cada término. A partir de allí, se introduce la regresión múltiple, incorporando el control de variables, el problema de la multicolinealidad y los modelos con interacción. El capítulo culmina con un breve acercamiento a los modelos lineales generalizados (GLM).

**Capítulo 9: Glosario de funciones útiles**

Este último capítulo funciona como un apéndice técnico. No está pensado para ser leído de principio a fin, sino como una guía de consulta rápida. Se organiza por bloques temáticos que incluyen funciones para vectores, data frames, limpieza de datos, manipulación con `dplyr`, visualización con `ggplot2`, paquetes complementarios a `ggplot2` para personalizar gráficos, y análisis exploratorio con `dlookr`. Se busca que el lector pueda encontrar rápidamente el código o función que necesita aplicar en su propio proyecto.

Adicionalmente, verás que a lo largo del libro, he incluido ejemplos inspirados cuestiones típicas de un investigador social los cuales están protagonizados por **Flor**, un personaje diseñado para acompañar al lector en los primeros capítulos y mostrar, desde las ciencias sociales, cómo aplicar las herramientas estadísticas para resolver problemas concretos.

Finalmente, mi objetivo no es que repitas fórmulas ni copies código: quiero que comprendas qué hace cada herramienta, por qué es adecuada en determinado contexto y cuáles son sus limitaciones. Al final, lo que importa no es ejecutar scripts sin errores —porque los vas a cometer, y muchos—, sino construir argumentos sólidos a partir de datos.
