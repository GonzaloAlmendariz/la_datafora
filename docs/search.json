[
  {
    "objectID": "sesion1.html#objetivos-de-la-sesión-de-hoy",
    "href": "sesion1.html#objetivos-de-la-sesión-de-hoy",
    "title": "Sesión 1: Introducción",
    "section": "Objetivos de la sesión de hoy",
    "text": "Objetivos de la sesión de hoy\nFamiliarizarse con el entorno R y RStudio, comprender la estructura básica del lenguaje, instalar y cargar paquetes, e importar bases de datos en diversos formatos. Primer contacto con la exploración estructural de datos."
  },
  {
    "objectID": "sesion1.html#crear-un-archivo-de-script",
    "href": "sesion1.html#crear-un-archivo-de-script",
    "title": "Sesión 1: Introducción",
    "section": "Crear un archivo de script",
    "text": "Crear un archivo de script\nCuando trabajamos en R, utilizamos diferentes tipos de archivos para organizar y guardar nuestro trabajo. Dos formatos comunes son el archivo R Script y el documento Quarto. Ambos se utilizan para escribir código, pero cumplen propósitos diferentes que es importante entender.\nUn archivo R Script es un archivo simple donde escribimos y guardamos nuestras instrucciones de código. Sirve como un registro de los comandos que ejecutamos, permitiendo reutilizarlos o ajustarlos más adelante. Sin embargo, este tipo de archivo no incluye espacio para explicaciones extensas ni muestra los resultados directamente junto al código.\nEn cambio, un documento Quarto va más allá al permitir combinar texto explicativo, bloques de código y los resultados generados (como tablas y gráficos) en un solo archivo. Además, es posible exportar el archivo final en formatos como HTML, PDF o Word, haciéndolo ideal para documentar y compartir análisis de manera profesional (de hecho, este libro esta hecho en Quarto). Esta capacidad de integrar explicación, análisis y presentación en un mismo lugar hace que Quarto sea particularmente útil para aprender y comunicar análisis de datos. En nuestras clases, vamos a utilizar documentos Quarto.\n\nAccede a la ventana Files:\n\nEn RStudio, localiza el panel Files. Este panel muestra el contenido de la carpeta de trabajo que configuraste previamente.\n\nCrea un nuevo documento Quarto:\n\nEn el panel Files, haz clic en el botón New File y selecciona Quarto Document.\n\n\n\nAparecerá un cuadro de diálogo donde puedes configurar el título del documento, tu nombre y el formato de salida inicial (HTML es una buena opción para empezar). Haz clic en Create.\n\nGuarda automáticamente en la carpeta de trabajo:\n\nCuando crees el documento desde la ventana Files, este se guardará automáticamente en tu carpeta de trabajo configurada. No necesitas realizar pasos adicionales para seleccionar la ubicación.\n\n\nEn un archivo Quarto puedes combinar texto explicativo (en formato Markdown) con bloques (chunks) de código en R. Los bloques de código son las secciones donde escribirás las instrucciones que deseas ejecutar, y están delimitados por tres backticks (```) seguidos del lenguaje que estás utilizando (en este caso, r). Para agregar un bloque de código de manera rápida, puedes utilizar el atajo de teclado:\n\nCtrl + Alt + I en Windows y Linux.\nCmd + Option + I en Mac.\n\nEste atajo insertará automáticamente un nuevo chunk en tu documento, con la estructura básica para que puedas empezar a escribir tu código. Un chunk de código tiene este formato:\n\n# Aquí escribes tu código en R\n\nDentro del chunk, puedes incluir cualquier instrucción que se ejecutará cuando proceses el documento. Esto permite mantener el texto explicativo y el código separados pero integrados en el mismo archivo.\n\n\n\n\n\n\nImportante\n\n\n\nEs primordial que trabajes directamente desde tu carpeta de trabajo, asegurando que todos los archivos relacionados con el proyecto estén organizados en una carpeta."
  },
  {
    "objectID": "sesion1.html#operaciones-básicas-con-elementos",
    "href": "sesion1.html#operaciones-básicas-con-elementos",
    "title": "Sesión 1: Introducción",
    "section": "Operaciones básicas con elementos",
    "text": "Operaciones básicas con elementos\nEn R, al igual que en otros lenguajes de programación, es fundamental comprender cómo manipular distintos tipos de datos mediante operaciones básicas.\n\nOperadores matemáticos:\nPermiten realizar cálculos aritméticos entre valores.\n\n\\(a + b\\) → a + b\n\n\\(a - b\\) → a - b\n\n\\(a \\times b\\) → a * b\n\n\\(\\frac{a}{b}\\) → a / b\n\n\\(a^b\\) → a ^ b\n\n\\(\\sqrt{a}\\) → sqrt(a)\n\nSuma\n\n5 + 3\n\n[1] 8\n\n\nResta\n\n10 - 4\n\n[1] 6\n\n\nMultiplicación\n\n5*6\n\n[1] 30\n\n\nDivisión\n\n30/5\n\n[1] 6\n\n\nPotencia:\n\n5^2\n\n[1] 25\n\n\nOperaciones en paréntesis\n\n(2 + 3) * 10 \n\n[1] 50\n\n\nLos elementos lógicos son tratados como 0 y 1\n\nFALSE + FALSE\n\n[1] 0\n\n\n\nTRUE + TRUE\n\n[1] 2\n\n\n\nTRUE + FALSE\n\n[1] 1\n\n\n\n\n\n\n\n\nCuidado\n\n\n\nLos elementos de texto no están diseñados para realizar operaciones aritméticas como suma o multiplicación. Si intentas hacerlo, obtendrás un error. Esto se debe a que, a diferencia de los números, el texto no representa cantidades numéricas sobre las que se pueda operar.\n\n\nError:\n\"Hola\" + \"Que tal\"\nError in “Hola” + “Que tal” : non-numeric argument to binary operator\n\n\nOperadores de comparación\nPermiten evaluar relaciones entre valores. Devuelven siempre un valor lógico: TRUE o FALSE.\n\n\\(x = y\\) → x == y: Igualdad\n\\(x \\neq y\\) → x != y: Desigualdad\n\\(x &lt; y\\): Menor que\n\\(x \\leq y\\) → x &lt;= y: Menor o igual que\n\\(x &gt; y\\): Mayor que\n\\(x \\geq y\\) → x &gt;= y: Mayor o igual que\n\n\n5 == 5   \n\n[1] TRUE\n\n\n\n3 != 2    \n\n[1] TRUE\n\n\n\n10 &lt; 8    \n\n[1] FALSE\n\n\n\n4 &gt;= 4     \n\n[1] TRUE\n\n\n\n\nOperadores lógicos\nPermiten combinar condiciones. Estas expresiones también devuelven TRUE o FALSE.\n\nNegación lógica: \\(\\lnot x\\) → !x Invierte el valor lógico de x.\nConjunción lógica (Y): \\(x \\land y\\) → x & y Devuelve TRUE solo si ambas condiciones son verdaderas.\nDisyunción lógica (O): \\(x \\lor y\\) → x | y Devuelve TRUE si al menos una condición es verdadera.\n\nNegación lógica\n\n!TRUE    \n\n[1] FALSE\n\n!FALSE    \n\n[1] TRUE\n\n\nConjunción lógica (Y)\n\n  (5 &gt; 3) & (2 &lt; 4)   \n\n[1] TRUE\n\n  (10 &gt; 3) & (1 &gt; 5)  \n\n[1] FALSE\n\n\nDisyunción lógica (O):\n\n  (4 &lt; 2) | (7 &gt; 1)   \n\n[1] TRUE\n\n  (3 == 5) | (2 &gt; 10) \n\n[1] FALSE"
  },
  {
    "objectID": "sesion1.html#tipos-de-objetos-en-r",
    "href": "sesion1.html#tipos-de-objetos-en-r",
    "title": "Sesión 1: Introducción",
    "section": "Tipos de objetos en R",
    "text": "Tipos de objetos en R\nEn R, no todo es solo un número o una palabra. Muchas veces necesitamos guardar varios elementos juntos, y para eso usamos diferentes tipos de objetos. Cada uno tiene su propia estructura y sirve para distintos propósitos.\n\nLos principales tipos de objetos son:\nVectores\nConjunto de elementos del mismo tipo (todos números, o todos textos, o todos lógicos).\n\nedades = c(18, 21, 25)\nnombres = c(\"Ana\", \"Luis\", \"María\")\n\n\nedades\n\n[1] 18 21 25\n\nnombres\n\n[1] \"Ana\"   \"Luis\"  \"María\"\n\n\nMatrices\nComo una tabla, pero con solo un tipo de dato (por ejemplo, solo números).\n\nmatriz = matrix(1:6, nrow = 2)\n\n\nmatriz\n\n     [,1] [,2] [,3]\n[1,]    1    3    5\n[2,]    2    4    6\n\n\nListas\nConjunto de elementos que pueden ser de distintos tipos y formas.\n\nmi_lista = list(nombre = \"Andrés\", edades, aprobado = TRUE)\n\n\nmi_lista\n\n$nombre\n[1] \"Andrés\"\n\n[[2]]\n[1] 18 21 25\n\n$aprobado\n[1] TRUE\n\n\nData frames\nLa estructura más usada para bases de datos. Cada columna es un vector y puede tener un tipo diferente (números, textos, etc.).\n\ndatos = data.frame(nombre = c(\"Ana\", \"Luis\"), edad = c(18, 21),\n                   aprobado = c(T, F), horas_estudio = c(15, 9))\n\n\ndatos\n\n  nombre edad aprobado horas_estudio\n1    Ana   18     TRUE            15\n2   Luis   21    FALSE             9\n\n\nAl trabajar con datos en R, no solo importa tener la información: también importa cómo está organizada. Los objetos nos permiten guardar, transformar y analizar datos de formas muy distintas. Por eso, conocer sus tipos es esencial.\n\n\n\n\n\n\nDato\n\n\n\nCada tipo de objeto cumple un rol distinto:\n\nUn vector te permite guardar variables como edad, ingresos o respuestas a una encuesta.\nUn data frame organiza esas variables como columnas, ideal para trabajar con bases de datos reales.\nUna lista puede guardar resultados de modelos, gráficos o varios tipos de objetos juntos."
  },
  {
    "objectID": "sesion1.html#funciones-estructurales",
    "href": "sesion1.html#funciones-estructurales",
    "title": "Sesión 1: Introducción",
    "section": "Funciones estructurales",
    "text": "Funciones estructurales\nLas funciones estructurales te dicen cómo está armado un objeto. No transforman los datos, sino que te dan información sobre su forma interna.\nstr(): estructura interna\n\nstr(iris) \n\n'data.frame':   150 obs. of  5 variables:\n $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ...\n\n\nMuestra la estructura del objeto: cuántas filas tiene, qué variables contiene, qué tipo de datos hay en cada columna, y un vistazo a los primeros valores.\nhead() y tail(): primeras o últimas filas\n\nhead(iris)   # Muestra las primeras 6 filas\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\n\nTambién puedes pedir un número específico:\n\nhead(iris, 3)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n\n\n\ntail(iris)   # Muestra las últimas 6 filas\n\n    Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\n145          6.7         3.3          5.7         2.5 virginica\n146          6.7         3.0          5.2         2.3 virginica\n147          6.3         2.5          5.0         1.9 virginica\n148          6.5         3.0          5.2         2.0 virginica\n149          6.2         3.4          5.4         2.3 virginica\n150          5.9         3.0          5.1         1.8 virginica\n\n\nDe la misma forma con tail():\n\nhead(iris, 3)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n\n\nnrow() y ncol(): número de filas y columnas\n\nnrow(iris)   # Cuántas filas hay (observaciones)\n\n[1] 150\n\nncol(iris)   # Cuántas columnas tiene (variables)\n\n[1] 5\n\n\ncolnames(): nombres de columnas\n\ncolnames(iris)\n\n[1] \"Sepal.Length\" \"Sepal.Width\"  \"Petal.Length\" \"Petal.Width\"  \"Species\"     \n\n\nDevuelve un vector con los nombres de todas las columnas.\n\n\n\n\n\n\nImportante\n\n\n\nEstas funciones te permiten entender rápidamente con qué tipo de objeto estás trabajando, qué tiene dentro y cómo interactuar con él. Son como las funciones de “reconocimiento” antes de empezar a transformar o analizar."
  },
  {
    "objectID": "sesion1.html#funciones-analíticas",
    "href": "sesion1.html#funciones-analíticas",
    "title": "Sesión 1: Introducción",
    "section": "Funciones analíticas",
    "text": "Funciones analíticas\nLa funciones analíticas procesan los datos y te dan un resultado: una media, una tabla, un resumen, una visualización, etc.\nmean(): calcular el promedio\n\nmean(iris$Petal.Length)\n\n[1] 3.758\n\n\nNos da el promedio de la longitud de los pétalos. Como mean() espera un vector numérico, le pasamos solo una columna del data.frame: iris$Petal.Length.\nsummary(): resumen numérico del data.frame\n\nsummary(iris)\n\n  Sepal.Length   Sepal.Width    Petal.Length   Petal.Width        Species  \n Min.   :4.30   Min.   :2.00   Min.   :1.00   Min.   :0.1   setosa    :50  \n 1st Qu.:5.10   1st Qu.:2.80   1st Qu.:1.60   1st Qu.:0.3   versicolor:50  \n Median :5.80   Median :3.00   Median :4.35   Median :1.3   virginica :50  \n Mean   :5.84   Mean   :3.06   Mean   :3.76   Mean   :1.2                  \n 3rd Qu.:6.40   3rd Qu.:3.30   3rd Qu.:5.10   3rd Qu.:1.8                  \n Max.   :7.90   Max.   :4.40   Max.   :6.90   Max.   :2.5                  \n\n\nAplica una función a todas las columnas del data.frame. Muestra medias, medianas y rangos para columnas numéricas, y conteos para las categóricas.\ntable(): resumen de frecuencias\n\ntable(iris$Species)\n\n\n    setosa versicolor  virginica \n        50         50         50 \n\n\nplot nos permite crear una visualización de forma sencilla\n\nplot(iris$Petal.Length, iris$Petal.Width,\n     col = iris$Species,\n     pch = 19,\n     xlab = \"Largo del pétalo\",\n     ylab = \"Ancho del pétalo\",\n     main = \"Pétalo: largo vs ancho por especie\")"
  },
  {
    "objectID": "sesion1.html#instalar-un-paquete",
    "href": "sesion1.html#instalar-un-paquete",
    "title": "Sesión 1: Introducción",
    "section": "Instalar un paquete",
    "text": "Instalar un paquete\nMe gusta pensar que instalar un paquete en R es similar a ir a la tienda y comprar una caja de herramientas. Después de comprarla, la mueves a tu propio almacén para asegurarte de que la tendrás cuando la necesites. Esto solo necesita hacerse una vez por paquete, a menos que quieras actualizarlo (tener la última versión).\nCon R, instalamos un paquete con una función llamada install.packages() seguida del nombre del paquete entre comillas.\n\ninstall.packages('dplyr')"
  },
  {
    "objectID": "sesion1.html#cargar-un-paquete",
    "href": "sesion1.html#cargar-un-paquete",
    "title": "Sesión 1: Introducción",
    "section": "Cargar un paquete",
    "text": "Cargar un paquete\nCuando cargas un paquete en R, es como si sacaras la caja de herramientas del almacén y la pusieras en tu mesa de trabajo. Solo entonces las herramientas dentro de la caja te son accesibles para tus proyectos. Haces esto cada vez que comienzas un nuevo documento o una sesión de R.\nPara hacerlo, usamos la función library().\n\nlibrary(dplyr)\n\nTambién puedes lograr esto con :: si quieres usar una función completa de otro paquete, pero no quieres cargar el paquete completo. Esto es como sacar una herramienta específica de la caja, pero no poner toda la caja en tu mesa de trabajo.\nPor ejemplo, el paquete psych tiene una función describe que produce un resumen estadístico detallado de un conjunto de datos dado. Si queremos aplicarlo en un conjunto de datos predeterminado en R como iris, podemos hacerlo sin cargar todo psych:\n\npsych::describe(iris)\n\nProporciona un resumen estadístico de los datos: media, desviación estándar, mínimo, máximo, etc., todo sin tener que cargar todo el paquete psych.\n\n\n               n mean   sd median min max   se\nSepal.Length 150 5.84 0.83   5.80 4.3 7.9 0.07\nSepal.Width  150 3.06 0.44   3.00 2.0 4.4 0.04\nPetal.Length 150 3.76 1.77   4.35 1.0 6.9 0.14\nPetal.Width  150 1.20 0.76   1.30 0.1 2.5 0.06\nSpecies*     150 2.00 0.82   2.00 1.0 3.0 0.07\n\n\nglimpse() es una función de dplyr que proporciona una vista compacta del dataframe, mostrando los nombres de las variables, sus tipos de datos y una muestra de valores.\n\nglimpse(iris)\n\nRows: 150\nColumns: 5\n$ Sepal.Length &lt;dbl&gt; 5.1, 4.9, 4.7, 4.6, 5.0, 5.4, 4.6, 5.0, 4.4, 4.9, 5.4, 4.…\n$ Sepal.Width  &lt;dbl&gt; 3.5, 3.0, 3.2, 3.1, 3.6, 3.9, 3.4, 3.4, 2.9, 3.1, 3.7, 3.…\n$ Petal.Length &lt;dbl&gt; 1.4, 1.4, 1.3, 1.5, 1.4, 1.7, 1.4, 1.5, 1.4, 1.5, 1.5, 1.…\n$ Petal.Width  &lt;dbl&gt; 0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.3, 0.2, 0.2, 0.1, 0.2, 0.…\n$ Species      &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setosa, s…\n\n\nRecuerda:\n\nCuando instalas un paquete, estás tomando la caja de herramientas y asegurándola en tu propio almacén.\nCargar un paquete, entonces, es poner esa caja de herramientas en tu mesa de trabajo para que puedas usar sus herramientas.\nImportar una función del paquete es como sacar una herramienta de la caja."
  },
  {
    "objectID": "sesion1.html#manejo-de-valores-faltantes",
    "href": "sesion1.html#manejo-de-valores-faltantes",
    "title": "Sesión 1: Introducción",
    "section": "Manejo de valores faltantes",
    "text": "Manejo de valores faltantes\nEl manejo de valores faltantes es uno de los aspectos más complejos en la limpieza de datos, y un tema importante a considerar al trabajar con conjuntos de datos. Un valor perdido o NA en R no es lo mismo que un 0 o un espacio vacío. Un valor perdido (o NA, que significa “Not Available”) es una celda que no contiene información en absoluto, lo que puede ocurrir por diversas razones, como un error en la recolección de los datos, una respuesta no proporcionada en una encuesta o una omisión involuntaria al momento de ingresar los datos.\nPor ejemplo:\n\nencuesta_excel\n\n# A tibble: 10 × 3\n  genero    medio_comunicación  edad\n  &lt;chr&gt;     &lt;chr&gt;              &lt;dbl&gt;\n1 Masculino Televisión            34\n2 Femenino  Redes sociales        NA\n3 Femenino  Redes sociales        55\n4 Otro      Radio                 63\n5 Femenino  Televisión            NA\n6 Masculino Redes sociales        19\n# ℹ 4 more rows\n\n\nPuedes detectar estos valores la función como is.na(), que devuelve un valor lógico (TRUE o FALSE) indicando si un valor es NA. Seleccionamos la columna.\n\nis.na(encuesta_excel$edad)\n\n [1] FALSE  TRUE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE\n\n\nRecuerda que puedes sumar un vector lógico para contar los TRUE. En este caso los valores perdidos.\n\n# Cantidad de valores perdidos \nsum(is.na(encuesta_excel$edad)) \n\n[1] 2\n\n\nUna de las formas más simples de manejar valores faltantes es eliminarlos por completo. Esto puede hacerse utilizando la función drop_na() del paquete tidyr, que elimina las filas que contienen al menos un valor NA en cualquier columna. Esta es una solución rápida, pero es importante ser cauteloso, ya que puede resultar en la pérdida de información valiosa si hay muchos datos faltantes.\n\n# Cargamos tidyr\nlibrary(tidyr)\n\n\n# Eliminamos filas con valores faltantes\ndrop_na(encuesta_excel)\n\n# A tibble: 7 × 3\n  genero    medio_comunicación  edad\n  &lt;chr&gt;     &lt;chr&gt;              &lt;dbl&gt;\n1 Masculino Televisión            34\n2 Femenino  Redes sociales        55\n3 Otro      Radio                 63\n4 Masculino Redes sociales        19\n5 Masculino Periódico             75\n6 Femenino  Redes sociales        55\n# ℹ 1 more row\n\n\nComparemos\n\n# Podemos nombrarlo\nencuesta_sin_na = drop_na(encuesta_excel)\n\nPresta atención a las dimensiones del tibble original y del tibble sin NA\n\ndim(encuesta_excel)\n\n[1] 10  3\n\ndim(encuesta_sin_na)\n\n[1] 7 3\n\n\nSi queremos ser más específicos y eliminar valores faltantes solo en una columna particular, podemos usar:\n\n# Eliminamos filas donde la columna 'edad' tiene NA\nencuesta_sin_na = drop_na(encuesta_excel, edad)\n\nencuesta_sin_na\n\n# A tibble: 8 × 3\n  genero    medio_comunicación  edad\n  &lt;chr&gt;     &lt;chr&gt;              &lt;dbl&gt;\n1 Masculino Televisión            34\n2 Femenino  Redes sociales        55\n3 Otro      Radio                 63\n4 Masculino Redes sociales        19\n5 Masculino &lt;NA&gt;                  29\n6 Masculino Periódico             75\n# ℹ 2 more rows\n\n\n\n\n\n\n\n\nCuidado\n\n\n\nAunque eliminar valores faltantes puede ser un enfoque válido en algunos casos, no siempre es ideal. Si eliminamos demasiadas filas, podemos perder una cantidad significativa de información, lo que podría alterar los resultados de nuestro análisis. Por eso, en lugar de eliminar, muchas veces es preferible imputar los valores faltantes, es decir, reemplazarlos con un valor estimado. Por ejemplo, algunas estrategias comunes para imputar valores incluyen reemplazar por el promedio en el caso de variables numéricas o reemplazar por la moda en variables categóricas donde los valores faltantes pueden ser reemplazados por el valor más frecuente (la moda).\n\n\nPara las siguentes técnicas de limpieza utilizaremos el conjunto de datos encuesta_problematica. Te animo a que puedas identificar que posible problemas tiene antes de seguir adelante.\n\ndatos_problema = read.csv('encuesta_problematica.csv')\n\ndatos_problema\n\n  ID Nombre.PARTICIPANTE EDAD..años. Ingreso.Mensual género\n1  1         Luis Bartra          23            1500      F\n2  2        Carlos Gómez          27            2000      M\n3  2        Carlos Gómez          27            2000      M\n4  3         María López          45            2500      F\n5  4       Javier Muller          35            3800      M\n6  5          Liz García          29            1800      M"
  },
  {
    "objectID": "sesion1.html#estandarización-de-nombres-de-columnas",
    "href": "sesion1.html#estandarización-de-nombres-de-columnas",
    "title": "Sesión 1: Introducción",
    "section": "Estandarización de nombres de columnas",
    "text": "Estandarización de nombres de columnas\nLos nombres de las columnas en los conjuntos de datos pueden ser inconsistentes, incluir caracteres especiales o espacios que dificulten el manejo en R.\nPrimero, identificamos los nombres originales del conjunto de datos utilizando la función base names():\n\ncolnames(datos_problema)\n\n[1] \"ID\"                  \"Nombre.PARTICIPANTE\" \"EDAD..años.\"        \n[4] \"Ingreso.Mensual\"     \"género\"             \n\n\nPara resolver esto, utilizamos la función clean_names() del paquete janitor. Esta función transforma automáticamente los nombres de las columnas en un formato limpio y consistente:\n\n# Cargar el paquete janitor\nlibrary(janitor)\n\n# Estandarizar los nombres de las columnas\ndatos_problema = clean_names(datos_problema)\n\n\ncolnames(datos_problema)\n\n[1] \"id\"                  \"nombre_participante\" \"edad_anos\"          \n[4] \"ingreso_mensual\"     \"genero\"             \n\nhead(datos_problema, 3)\n\n  id nombre_participante edad_anos ingreso_mensual genero\n1  1         Luis Bartra        23            1500      F\n2  2        Carlos Gómez        27            2000      M\n3  2        Carlos Gómez        27            2000      M"
  },
  {
    "objectID": "sesion1.html#eliminación-de-duplicados",
    "href": "sesion1.html#eliminación-de-duplicados",
    "title": "Sesión 1: Introducción",
    "section": "Eliminación de duplicados",
    "text": "Eliminación de duplicados\nEn algunos casos, los datos pueden contener filas duplicadas que distorsionan el análisis. Podemos identificar y eliminar estas filas utilizando la función distinct() de dplyr.\n\n# Vemos el problema\ndatos_problema\n\n  id nombre_participante edad_anos ingreso_mensual genero\n1  1         Luis Bartra        23            1500      F\n2  2        Carlos Gómez        27            2000      M\n3  2        Carlos Gómez        27            2000      M\n4  3         María López        45            2500      F\n5  4       Javier Muller        35            3800      M\n6  5          Liz García        29            1800      M\n\n# Eliminamos filas duplicadas\ndatos_limpios = distinct(datos_problema)\n\n\nnrow(datos_problema)     # Antes: 7 filas\n\n[1] 6\n\nnrow(datos_limpios) \n\n[1] 5\n\n\nMostramos:\n\ndatos_limpios\n\n  id nombre_participante edad_anos ingreso_mensual genero\n1  1         Luis Bartra        23            1500      F\n2  2        Carlos Gómez        27            2000      M\n3  3         María López        45            2500      F\n4  4       Javier Muller        35            3800      M\n5  5          Liz García        29            1800      M"
  },
  {
    "objectID": "sesion1.html#guardar-como-archivo-.rds",
    "href": "sesion1.html#guardar-como-archivo-.rds",
    "title": "Sesión 1: Introducción",
    "section": "Guardar como archivo .RDS",
    "text": "Guardar como archivo .RDS\nEl formato .RDS es propio de R. Guarda un solo objeto, como un data.frame, conservando su estructura interna.\n\nsaveRDS(datos_limpios, file = \"datos_limpios.rds\")\n\nLuego, puedes volver a cargarlo con:\n\ndatos_limpios = readRDS(\"datos_limpios.rds\")\n\n\n\n\n\n\n\nDato\n\n\n\nEsto es ideal cuando trabajas con datos procesados dentro de R y quieres retomarlos luego sin tener que rehacer todo el procesamiento."
  },
  {
    "objectID": "sesion1.html#exportar-como-archivo-.csv",
    "href": "sesion1.html#exportar-como-archivo-.csv",
    "title": "Sesión 1: Introducción",
    "section": "Exportar como archivo .csv",
    "text": "Exportar como archivo .csv\nPara compartir o abrir en Excel u otros programas, lo más común es exportar a CSV.\n\nwrite.csv(datos_limpios, \n          file = \"datos_limpios.csv\", \n          row.names = FALSE)\n\n\nrow.names = FALSE evita que R agregue una columna extra con los números de fila."
  },
  {
    "objectID": "sesion1.html#guardar-todo-el-entorno-de-trabajo",
    "href": "sesion1.html#guardar-todo-el-entorno-de-trabajo",
    "title": "Sesión 1: Introducción",
    "section": "Guardar todo el entorno de trabajo",
    "text": "Guardar todo el entorno de trabajo\nSi deseas guardar todos los objetos que tienes cargados en tu sesión (variables, data frames, modelos, etc.):\n\nsave.image(file = \"mi_entorno.RData\")\n\nY para volver a cargarlo en otra sesión de R:\n\nload(\"mi_entorno.RData\")\n\n\n\n\n\n\n\nImportante\n\n\n\nEs buena práctica guardar tus objetos con nombres claros y organizarlos en carpetas por cada proyecto a realizar. Esto no solo garantiza el orden entre disntintos proyectos, sino que mejora la reproducibilidad y facilita compartir tu análisis con otras personas o retomarlo en el futuro."
  },
  {
    "objectID": "sesion3.html",
    "href": "sesion3.html",
    "title": "Sesión 3: Visualización de datos con ggplot2",
    "section": "",
    "text": "Dominar la lógica de la gramática de gráficos implementada en ggplot2. Construir gráficos claros, funcionales y personalizables para la exploración de variables numéricas y categóricas. Componer visualizaciones múltiples y utilizar recursos de diseño.\nPoder visualizar la información no solo nos ayuda a a anticipar ciertos patrones y comportamientos, sino que también facilita la comunicación de estos hallazgos a otros. Es importante reconocer que tendemos a dejarnos llevar más por los gráficos que por los resúmenes numéricos, aunque ambos sean igualmente importantes."
  },
  {
    "objectID": "sesion3.html#objetivo-de-la-sesión-de-hoy",
    "href": "sesion3.html#objetivo-de-la-sesión-de-hoy",
    "title": "Sesión 3: Visualización de datos con ggplot2",
    "section": "",
    "text": "Dominar la lógica de la gramática de gráficos implementada en ggplot2. Construir gráficos claros, funcionales y personalizables para la exploración de variables numéricas y categóricas. Componer visualizaciones múltiples y utilizar recursos de diseño.\nPoder visualizar la información no solo nos ayuda a a anticipar ciertos patrones y comportamientos, sino que también facilita la comunicación de estos hallazgos a otros. Es importante reconocer que tendemos a dejarnos llevar más por los gráficos que por los resúmenes numéricos, aunque ambos sean igualmente importantes."
  },
  {
    "objectID": "sesion3.html#y-ahora-simplemente-podemos-sumarle-una-capa-theme",
    "href": "sesion3.html#y-ahora-simplemente-podemos-sumarle-una-capa-theme",
    "title": "Sesión 3: Visualización de datos con ggplot2",
    "section": "Y ahora, simplemente podemos sumarle una capa theme:",
    "text": "Y ahora, simplemente podemos sumarle una capa theme:\ntheme_minimal() crea un gráfico con fondo blanco y cuadrícula ligera:\n\ngrafico + theme_minimal()\n\n\n\n\n\n\n\n\ntheme_gray() es el tema por defecto de ggplot2: fondo gris claro y cuadrícula blanca.\n\ngrafico + theme_gray()\n\n\n\n\n\n\n\n\ntheme_bw() tiene fondo blanco y líneas más marcadas. Es útil si se imprimirá el gráfico en blanco y negro.\n\ngrafico + theme_bw()\n\n\n\n\n\n\n\n\ntheme_classic() imita el estilo clásico de los gráficos base de R.\n\ngrafico + theme_classic()\n\n\n\n\n\n\n\n\ntheme_void() elimina todos los elementos del gráfico, dejando solo los datos.\n\ngrafico + theme_void()\n\n\n\n\n\n\n\n\ntheme_dark() aplica un fondo oscuro con líneas claras.\n\ngrafico + theme_dark()\n\n\n\n\n\n\n\n\nTodos estos temas pueden ser personalizados si necesitas ajustes más detallados. En esta sesión usaremos theme_minimal() por preferencia, pero puedes elegir el que mejor se adapte a tu estilo o propósito."
  },
  {
    "objectID": "sesion3.html#histograma",
    "href": "sesion3.html#histograma",
    "title": "Sesión 3: Visualización de datos con ggplot2",
    "section": "Histograma",
    "text": "Histograma\nEl histograma permite observar la distribución de una variable numérica. Por ejemplo, ¿cómo están distribuidos los valores de esperanza de vida?\n\nggplot(df_22, aes(x = esp_vida)) +\n  geom_histogram(binwidth = 5, fill = \"skyblue\", color = \"black\") +\n  labs(title = \"Distribución de la esperanza de vida\",\n       x = \"Esperanza de vida\",\n       y = \"Frecuencia\") +\n  theme_minimal()"
  },
  {
    "objectID": "sesion3.html#gráfico-de-líneas",
    "href": "sesion3.html#gráfico-de-líneas",
    "title": "Sesión 3: Visualización de datos con ggplot2",
    "section": "Gráfico de líneas",
    "text": "Gráfico de líneas\nUn gráfico de líneas es útil para ver cómo cambia una variable a lo largo del tiempo. Si tuvieras datos de varios años, podrías agrupar por año.\nPodemos combinar dplyr con ggplot2 utilizando el operador pipe (%&gt;%) para encadenar directamente una transformación de datos con una visualización. Esto nos permite filtrar, agrupar o resumir datos con dplyr, y luego pasar el resultado directamente a ggplot() para graficarlo, sin necesidad de guardar objetos intermedios.\n\ndf %&gt;%\n  filter(region != \"Aggregates\") %&gt;% \n  group_by(year, region) %&gt;%\n  reframe(esp_vida_media = mean(esp_vida, na.rm = TRUE)) %&gt;%\n  ggplot(aes(x = year, y = esp_vida_media, color = region)) +\n  geom_line(size = 1) +\n  labs(title = \"Evolución de la esperanza de vida por región\",\n       x = \"Año\",\n       y = \"Esperanza de vida (promedio)\",\n       color = \"Región\") +\n  theme_minimal() +\n  theme(legend.position = \"top\")"
  },
  {
    "objectID": "sesion3.html#boxplot",
    "href": "sesion3.html#boxplot",
    "title": "Sesión 3: Visualización de datos con ggplot2",
    "section": "Boxplot",
    "text": "Boxplot\nUn boxplot muestra cómo se distribuye una variable numérica en distintas categorías. Por ejemplo, la distribución de la esperanza de vida según región:\n\ndf_22 %&gt;% \n  ggplot(aes(x = region, y = esp_vida, fill = region)) +\n  geom_boxplot() +\n  labs(title = \"Distribución de la esperanza de vida por región\",\n       x = \"Región\",\n       y = \"Esperanza de vida\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")"
  },
  {
    "objectID": "sesion3.html#gráfico-de-barras",
    "href": "sesion3.html#gráfico-de-barras",
    "title": "Sesión 3: Visualización de datos con ggplot2",
    "section": "Gráfico de barras",
    "text": "Gráfico de barras\nPermite ver la frecuencia de cada categoría en barras\n\ndf_22 %&gt;%\n  ggplot(aes(x = region, fill = region)) +\n  geom_bar() +\n  labs(\n    title = \"Frecuencia de observaciones por región\",\n    x = \"Región\",\n    y = \"Frecuencia\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  coord_flip()"
  },
  {
    "objectID": "sesion3.html#gráfico-de-barras-con-valores-agregados",
    "href": "sesion3.html#gráfico-de-barras-con-valores-agregados",
    "title": "Sesión 3: Visualización de datos con ggplot2",
    "section": "Gráfico de barras con valores agregados",
    "text": "Gráfico de barras con valores agregados\nSi ya tienes una tabla resumida con frecuencias, o quieres mostrar un valor calculado como la esperanza de vida promedio por región, usamos geom_col(). También podemos usar reorder() para ordenar las barras de menor a mayor vida promedio.\n\ndf_22 %&gt;%\n  group_by(region) %&gt;%\n  reframe(vida_media = mean(esp_vida, na.rm = TRUE)) %&gt;%\n  ggplot(aes(x = reorder(region, vida_media), \n             y = vida_media, fill = region)) +\n  geom_col() +\n  labs(title = \"Esperanza de vida promedio en 2022\",\n       x = \"Región\",\n       y = \"Esperanza de vida\") +\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  coord_flip()"
  },
  {
    "objectID": "sesion3.html#gráfico-de-proporciones",
    "href": "sesion3.html#gráfico-de-proporciones",
    "title": "Sesión 3: Visualización de datos con ggplot2",
    "section": "Gráfico de proporciones",
    "text": "Gráfico de proporciones\nTambién podemos visualizar proporciones relativas entre categorías. Por ejemplo, la proporción de países por tipo de ingreso en cada región. position = \"fill\" transforma las barras en proporciones relativas\n\ndf_22 %&gt;%\n  ggplot(aes(x = region, fill = income)) +\n  geom_bar(position = \"fill\") +\n  labs(title = \"Distribución proporcional de ingresos por región\",\n       x = \"Región\",\n       y = \"Proporción\",\n       fill = \"Tipo de ingreso\") +\n  theme_minimal() +\n  scale_y_continuous(labels = scales::percent)\n\n\n\n\n\n\n\n\n\n\ndf_22$income no tiene un orden factorizado, ggplot2 lo tratará como categorías sin orden, lo cual puede resultar en un orden de los niveles arbitrario. Podemos ordenar explícitamente los niveles de income usando forcats::fct_relevel(). En este caso, estamos trabajando con categorías como:\n\nLow income\nLower middle income\nUpper middle income\nHigh income\n\nFactorizamos y establecemos orden entre los niveles:\n\ndf_22 &lt;- df_22 %&gt;%\n  filter(income != \"Not classified\") %&gt;%\n  mutate(income = forcats::fct_relevel(income,\n                                       \"Low income\",\n                                       \"Lower middle income\",\n                                       \"Upper middle income\",\n                                       \"High income\"))\n\n\ndf_22 %&gt;% \n  ggplot(aes(x = region, fill = income)) +\n  geom_bar(position = \"fill\") +\n  labs(title = \"Distribución proporcional de ingresos por región\",\n       x = \"Región\",\n       y = \"Proporción\",\n       fill = \"Tipo de ingreso\") +\n  theme_minimal() +\n  scale_y_continuous(labels = scales::percent)"
  },
  {
    "objectID": "sesion3.html#boxplot-comparando-distribuciones",
    "href": "sesion3.html#boxplot-comparando-distribuciones",
    "title": "Sesión 3: Visualización de datos con ggplot2",
    "section": "Boxplot: comparando distribuciones",
    "text": "Boxplot: comparando distribuciones\nEl boxplot resume la distribución de la variable numérica dentro de cada categoría. Podemos ver la mediana, los cuartiles y posibles valores atípicos.\n\ndf_22 %&gt;%\n  ggplot(aes(x = income, y = esp_vida, fill = income)) +\n  geom_boxplot() +\n  labs(title = \"Distribución de la esperanza de vida por tipo de ingreso\",\n       x = \"Tipo de ingreso\",\n       y = \"Esperanza de vida\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")"
  },
  {
    "objectID": "sesion3.html#violin-plot-forma-de-la-distribución",
    "href": "sesion3.html#violin-plot-forma-de-la-distribución",
    "title": "Sesión 3: Visualización de datos con ggplot2",
    "section": "Violin plot: forma de la distribución",
    "text": "Violin plot: forma de la distribución\nEl violin plot es parecido al boxplot, pero también muestra la densidad de los datos, es decir, qué tan concentrados están los valores.\n\ndf_22 %&gt;%\n  ggplot(aes(x = income, y = esp_vida, fill = income)) +\n  geom_violin(trim = FALSE, alpha = 0.7) +\n  labs(title = \"Distribución de la esperanza de vida por tipo de ingreso\",\n       x = \"Tipo de ingreso\",\n       y = \"Esperanza de vida\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\n\n\n\nNumérica vs numérica"
  },
  {
    "objectID": "sesion3.html#gráfico-de-dispersión-scatter-plot",
    "href": "sesion3.html#gráfico-de-dispersión-scatter-plot",
    "title": "Sesión 3: Visualización de datos con ggplot2",
    "section": "Gráfico de dispersión (scatter plot)",
    "text": "Gráfico de dispersión (scatter plot)\nIdeal para ver relaciones, correlaciones o agrupamientos.\n\nggplot(df_22, aes(x = pib_capita, y = esp_vida)) +\n  geom_point(alpha = 0.6) +\n  scale_x_log10() +\n  labs(x = \"PIB per cápita (log10)\", y = \"Esperanza de vida\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\nTambién puedes agregar una línea de tendencia con geom_smooth() si te interesa mostrar una relación.\n\nggplot(df_22, aes(x = pib_capita, y = esp_vida)) +\n  geom_point(alpha = 0.6) +\n  scale_x_log10() +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(x = \"PIB per cápita (log10)\", y = \"Esperanza de vida\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\nCategórica vs. categórica"
  },
  {
    "objectID": "sesion3.html#gráfico-de-barras-apilado-o-de-posición",
    "href": "sesion3.html#gráfico-de-barras-apilado-o-de-posición",
    "title": "Sesión 3: Visualización de datos con ggplot2",
    "section": "Gráfico de barras apilado o de posición",
    "text": "Gráfico de barras apilado o de posición\nCuando ambas variables son categóricas.\n\nggplot(df_22, aes(x = income, fill = region)) +\n  geom_bar(position = \"fill\") +\n  labs(x = \"Nivel de ingreso\", y = \"Proporción\", fill = \"Región\") +\n  theme_minimal()"
  },
  {
    "objectID": "sesion3.html#ggsci-paletas-inspiradas-en-publicaciones-científicas",
    "href": "sesion3.html#ggsci-paletas-inspiradas-en-publicaciones-científicas",
    "title": "Sesión 3: Visualización de datos con ggplot2",
    "section": "1. ggsci: paletas inspiradas en publicaciones científicas",
    "text": "1. ggsci: paletas inspiradas en publicaciones científicas\nEste paquete ofrece paletas inspiradas en revistas como Nature, NEJM, Lancet y JAMA.\n\nlibrary(ggsci)\n\n\nggplot(df_22, aes(x = region, y = esp_vida, fill = region)) +\n  geom_boxplot() +\n  scale_fill_nejm() +\n  theme_minimal() +\n  labs(title = \"Boxplot por región con paleta NEJM\")\n\n\n\n\n\n\n\n\n\n\n\n\nTambién puedes probar con scale_fill_lancet(), scale_fill_jama(), etc.\n\n\nggplot(df_22, aes(x = region, y = esp_vida, fill = region)) +\n  geom_boxplot() +\n  scale_fill_lancet() +\n  theme_minimal() +\n  labs(title = \"Boxplot por región con paleta Lancet\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(df_22, aes(x = region, y = esp_vida, fill = region)) +\n  geom_boxplot() +\n  scale_fill_jama() +\n  theme_minimal() +\n  labs(title = \"Boxplot por región con paleta Jama\")"
  },
  {
    "objectID": "sesion3.html#viridis-paletas-perceptualmente-uniformes",
    "href": "sesion3.html#viridis-paletas-perceptualmente-uniformes",
    "title": "Sesión 3: Visualización de datos con ggplot2",
    "section": "2. viridis: paletas perceptualmente uniformes",
    "text": "2. viridis: paletas perceptualmente uniformes\nIdeal para asegurar buena visibilidad incluso en escalas de grises o para personas con daltonismo.\n\nlibrary(viridis)\n\n\nggplot(df_22, aes(x = esp_vida, fill = region)) +\n  geom_histogram(binwidth = 3, color = \"black\") +\n  scale_fill_viridis_d(option = \"C\") +\n  facet_wrap(~ region) +\n  theme_minimal() +\n  labs(title = \"Histograma por región con viridis\",\n       x = '', y = '') +\n  theme(legend.position = 'none')"
  },
  {
    "objectID": "sesion3.html#patchwork-combinar-múltiples-gráficos",
    "href": "sesion3.html#patchwork-combinar-múltiples-gráficos",
    "title": "Sesión 3: Visualización de datos con ggplot2",
    "section": "3. patchwork: combinar múltiples gráficos",
    "text": "3. patchwork: combinar múltiples gráficos\nPermite apilar gráficos horizontal o verticalmente de forma muy intuitiva.\n\nlibrary(patchwork)\n\nPrimer gráfico: distribución general\n\np1 = ggplot(df_22, aes(x = esp_vida)) + \n  geom_density(fill = \"darkgreen\", alpha = 0.7) +\n  labs(title = \"Distribución de la esperanza de vida\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\nSegundo gráfico: boxplot por región\n\np2 = ggplot(df_22, aes(x = region, y = esp_vida)) + \n  geom_boxplot(fill = \"darkgreen\", alpha = 0.7) +\n  labs(title = \"Esperanza de vida por región\", x = \"Región\", y = \"Esperanza de vida\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\nTercer gráfico: violín por nivel de ingreso\n\np3 = df_22 %&gt;%\n  filter(income != \"Not classified\") %&gt;%\n  ggplot(aes(x = income, y = esp_vida)) + \n  geom_violin(fill = \"darkgreen\", alpha = 0.7) +\n  labs(title = \"Esperanza de vida por nivel de ingreso\", x = \"Nivel de ingreso\", y = \"Esperanza de vida\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\nCombinamos todos\n\n(p1 | p2) / p3"
  },
  {
    "objectID": "sesion3.html#ggthemes-estilos-inspirados-en-medios",
    "href": "sesion3.html#ggthemes-estilos-inspirados-en-medios",
    "title": "Sesión 3: Visualización de datos con ggplot2",
    "section": "4. ggthemes: estilos inspirados en medios",
    "text": "4. ggthemes: estilos inspirados en medios\nProporciona temas como los de The Economist o Wall Street Journal.\n\nlibrary(ggthemes)\n\n\n\nggplot(df_22, aes(x = region, y = esp_vida, fill = region)) +\n  geom_boxplot() +\n  theme_economist() +\n  scale_fill_economist() +\n  labs(title = \"Estilo The Economist\")"
  },
  {
    "objectID": "sesion3.html#ggtext-etiquetas-enriquecidas-con-htmlmarkdown",
    "href": "sesion3.html#ggtext-etiquetas-enriquecidas-con-htmlmarkdown",
    "title": "Sesión 3: Visualización de datos con ggplot2",
    "section": "5. ggtext: etiquetas enriquecidas con HTML/Markdown",
    "text": "5. ggtext: etiquetas enriquecidas con HTML/Markdown\nPermite títulos y etiquetas con formato enriquecido.\n\nlibrary(ggtext)\nlibrary(glue)\n\n\n\nunique(df_22$region)\n\n[1] \"South Asia\"                 \"Europe & Central Asia\"     \n[3] \"Middle East & North Africa\" \"East Asia & Pacific\"       \n[5] \"Sub-Saharan Africa\"         \"Latin America & Caribbean\" \n[7] \"North America\"             \n\n\nEntonces…\n\nDefinimos un vector colores que coincida exactamente con esos nombres:\n\ncolores = c(\n  \"Sub-Saharan Africa\" = \"#E63946\",\n  \"Middle East & North Africa\" = \"#F28482\",\n  \"South Asia\" = \"#2A9D8F\",\n  \"East Asia & Pacific\" = \"#8AC926\",\n  \"Europe & Central Asia\" = \"#F4A261\",\n  \"Latin America & Caribbean\" = \"#457B9D\",\n  \"North America\" = \"#A17DB2\"\n)\n\n\nUsamos glue() con HTML y los nombres reales de las regiones:\n\ntitulo = glue(\n  \"Esperanza de vida por región:&lt;br&gt;\",\n  \"&lt;span style='color:{colores['Sub-Saharan Africa']}'&gt;África Subsahariana&lt;/span&gt;, \",\n  \"&lt;span style='color:{colores['Middle East & North Africa']}'&gt;MENA&lt;/span&gt;, \",\n  \"&lt;span style='color:{colores['South Asia']}'&gt;Asia del Sur&lt;/span&gt;, \",\n  \"&lt;span style='color:{colores['East Asia & Pacific']}'&gt;Asia Oriental y Pacífico&lt;/span&gt;, \",\n  \"&lt;span style='color:{colores['Europe & Central Asia']}'&gt;Europa y Asia Central&lt;/span&gt;, \",\n  \"&lt;span style='color:{colores['Latin America & Caribbean']}'&gt;América Latina&lt;/span&gt;, \",\n  \"&lt;span style='color:{colores['North America']}'&gt;Norteamérica&lt;/span&gt;\"\n)\n\n\n\n# Reordenamos la variable region según el vector de colores\ndf_22 = df_22 %&gt;%\n  mutate(region = factor(region, levels = names(colores)))\n\n# Luego graficamos\nggplot(df_22, aes(x = region, y = esp_vida, fill = region)) +\n  geom_boxplot() +\n  scale_fill_manual(values = colores) +\n  theme_minimal() +\n  theme(\n    plot.title = element_markdown(),\n    legend.position = \"none\"\n  ) +\n  ggtitle(titulo)"
  },
  {
    "objectID": "sesion3.html#scales-etiquetas-numéricas-personalizadas",
    "href": "sesion3.html#scales-etiquetas-numéricas-personalizadas",
    "title": "Sesión 3: Visualización de datos con ggplot2",
    "section": "6. scales: etiquetas numéricas personalizadas",
    "text": "6. scales: etiquetas numéricas personalizadas\nPara mostrar cifras en formato legible (por ejemplo, moneda o porcentaje).\n\nlibrary(scales)\n\n\nggplot(df_22, aes(x = pib_capita, y = esp_vida)) +\n  geom_point(color = \"steelblue\") +\n  scale_x_continuous(labels = dollar_format(prefix = \"US$\")) +\n  theme_minimal() +\n  labs(title = \"Relación entre PIB per cápita y esperanza de vida\", \n       x = \"\", y = \"Esperanza de vida\")"
  },
  {
    "objectID": "sesion3.html#gghighlight-destacar-subconjuntos-de-datos",
    "href": "sesion3.html#gghighlight-destacar-subconjuntos-de-datos",
    "title": "Sesión 3: Visualización de datos con ggplot2",
    "section": "7. gghighlight: destacar subconjuntos de datos",
    "text": "7. gghighlight: destacar subconjuntos de datos\nSirve para resaltar observaciones relevantes en un gráfico.\n\nlibrary(gghighlight)\n\n\n\npaises_vecinos = c(\"Peru\", \"Brasil\", \"Colombia\", \n                   \"Ecuador\", \"Chile\", \"Argentina\")\n\n\n\ndf_22 %&gt;%\n  ggplot(aes(x = log(pib_capita), y = esp_vida, label = country)) +\n  geom_point(color = \"gray30\", size = 2) +\n  gghighlight(country %in% paises_vecinos,\n              label_key = country,\n              unhighlighted_params = list(alpha = 0.4)) +\n  theme_minimal() +\n  labs(\n    title = \"Países sudamericanos seleccionados resaltados\",\n    subtitle = \"Relación entre PBI per cápita (log) y esperanza de vida\",\n    x = \"PBI per cápita (log10)\",\n    y = \"Esperanza de vida\"\n  )"
  },
  {
    "objectID": "sesion4.html",
    "href": "sesion4.html",
    "title": "Sesión 4: Probabilidad, simulación, pruebas estadísticas y modelos de regresión",
    "section": "",
    "text": "Para analizar datos con criterio estadístico y formular conclusiones que vayan más allá de lo observado, es fundamental comprender qué implica hablar de probabilidad. Aunque su origen precede al desarrollo formal de la estadística, la probabilidad se ha convertido en uno de los pilares fundamentales para interpretar fenómenos aleatorios, cuantificar la incertidumbre y construir modelos que permitan realizar inferencias válidas a partir de una muestra.\nEn su libro Probability, Statistics, and Data: A Fresh Approach Using R, Speegle (2021) propone una definición precisa:\n\n“La probabilidad de un evento es un número entre cero y uno que describe la proporción de veces que se espera que dicho evento ocurra.”\n\nEste enfoque es frecuentista, es decir: entiende la probabilidad como un valor que resume lo que se espera en el largo plazo si un experimento se repite muchas veces.\nEsta idea sugiere que hablar de la probabilidad de un evento es cuantificar cuán probable es su ocurrencia, comparada con todos los resultados posibles. Al lanzar una moneda, la probabilidad de obtener sello es de \\(\\frac{1}{2}\\), lo que equivale a 0.5 o al 50%. Esta proporción puede visualizarse como una fracción de veces que se espera que ocurra el evento a lo largo de múltiples repeticiones del experimento.\nPodemos visualizar la probabilidad de este evento como una barra que representa el resultado del la moneda que nos interesa, en relación con el total de resultados posibles.\n\n\n\n\n\n\n\n\n\nAmbas barras representan la proporción del total de posibles resultados. En términos gráficos, la altura de la barra muestra la probabilidad de que ocurra cada resultado.\nAquí es donde el concepto de área tiene relevancia. La probabilidad de cada número puede visualizarse como el área de la barra en un gráfico, en comparación con el área total que representan todos los posibles resultados. Para la moneda, la probabilidad de obtener un sello es \\(\\frac{1}{2}\\), lo que significa que la barra para el resultado sello ocupa la mitad del área total del gráfico.\nPero, ¿qué pasa cuando no estamos tratando con eventos discretos, como el lanzamiento de una modena, sino con variables que pueden tomar un rango infinito de valores? Por ejemplo, si quisiéramos saber la probabilidad de que la altura de una persona esté entre 160 cm y 170 cm, no podemos representar esta situación con barras, porque los valores posibles son infinitos (puede haber alturas de 160.1 cm, 160.2 cm, etc.).\nEn estos casos, la probabilidad la representamos con una curva. Al igual que con las barras, el área bajo la curva nos muestra la probabilidad, pero en este caso, la probabilidad se calcula para un rango de valores. La idea sigue siendo la misma: la probabilidad es el área, pero ahora estamos mirando el área bajo una curva en lugar de barras separadas.\nPor ejemplo, si estamos interesados en la probabilidad de que una altura esté entre 160 cm y 170 cm, esa probabilidad sería el área bajo la curva entre esos dos puntos. Mientras que en el caso de las barras sabíamos exactamente cuántos resultados posibles había, en el caso de la curva no tenemos un número fijo de resultados, sino un rango, y el área debajo de ese rango nos da la probabilidad de que el valor caiga ahí."
  },
  {
    "objectID": "sesion4.html#probabilidad-experimental",
    "href": "sesion4.html#probabilidad-experimental",
    "title": "Sesión 4: Probabilidad, simulación, pruebas estadísticas y modelos de regresión",
    "section": "Probabilidad experimental",
    "text": "Probabilidad experimental\nVeamos qué tan cerca está la frecuencia observada del valor teórico. Comenzamos con 10 lanzamientos:\n\nset.seed(123)\nlanzamientos_10 = sample(1:6, 10, replace = TRUE)\nsum(lanzamientos_10 == 3) / 10\n\n[1] 0.3\n\n\n¿Qué ocurre si lanzamos el dado 100 veces?\n\nlanzamientos_100 &lt;- sample(1:6, 100, replace = TRUE)\nsum(lanzamientos_100 == 3) / 100\n\n[1] 0.15\n\n\n¿Y con 10,000 lanzamientos?\n\nlanzamientos_10000 &lt;- sample(1:6, 10000, replace = TRUE)\nsum(lanzamientos_10000 == 3) / 10000\n\n[1] 0.167\n\n\nA medida que aumenta el número de lanzamientos, la probabilidad experimental se aproxima al valor teórico: \\[P(3) = \\frac{1}{6} \\approx 0.1667\\]"
  },
  {
    "objectID": "sesion4.html#pasos-del-contraste-de-hipótesis",
    "href": "sesion4.html#pasos-del-contraste-de-hipótesis",
    "title": "Sesión 4: Probabilidad, simulación, pruebas estadísticas y modelos de regresión",
    "section": "Pasos del Contraste de Hipótesis",
    "text": "Pasos del Contraste de Hipótesis\nI. Planteamiento de Hipótesis:\nComenzamos formulando dos hipótesis opuestas:\n\nLa hipótesis nula (\\(H_0\\)) representa la idea de “no hay efecto” o “no hay diferencia”. Es nuestra afirmación de partida y sugiere que cualquier variación que veamos es simplemente producto de la variación aleatoria inherente a nuestra muestra.\n\nLa hipótesis alternativa (\\(H_1\\)) representa la afirmación que queremos evaluar. Indica que sí hay un efecto o diferencia en la población.\n\nLa hipótesis nula nos da un punto de referencia neutral contra el cual comparamos nuestros resultados para evaluar si son suficientemente distintos como para sugerir algo real.\nDependiendo de la dirección en la que buscamos evidencia contra \\(H_0\\) el análisis puede ser:\n\nPrueba bilateral (de dos colas): Se usa cuando queremos evaluar si hay una diferencia sin importar la dirección. Por ejemplo, si estudiamos el impacto de una nueva política pública en la tasa de empleo, \\(H_1\\) indicaría que la política cambia la tasa de empleo, ya sea aumentándola o reduciéndola. En este caso, consideramos ambos extremos de la distribución para determinar si el efecto observado es lo suficientemente inusual como para rechazar \\(H_0\\).\nPrueba unilateral (de una cola): Se usa cuando tenemos una expectativa clara sobre la dirección del efecto. Por ejemplo, si evaluamos si un programa de capacitación mejora las habilidades laborales, la hipótesis alternativa indicaría que el programa aumenta el nivel de habilidades (y no simplemente que lo cambia en cualquier dirección). Aquí, solo nos interesa una de las colas de la distribución.\n\nII. Establecimiento de la Distribución Nula\nIncluso si \\(H_0\\) es cierta, los resultados de una muestra pueden fluctuar debido a la variación aleatoria. Sin embargo, estas fluctuaciones siguen un patrón: la mayoría de los valores observados estarán cerca de la media de la distribución nula, y conforme nos alejamos de esta media, los valores se vuelven menos probables. Esto significa que, en cualquier prueba de hipótesis, necesitamos un criterio que nos ayude a determinar cuándo un resultado es lo suficientemente diferente de lo esperado bajo \\(H_0\\) como para considerarlo significativo. . Para hacer esta comparación, necesitamos una distribución de referencia que nos muestre cómo se comportaría nuestro estadístico (como la media o la proporción) si la hipótesis nula fuera verdadera.\nAl igual que podemos construir una distribución de posibles valores para el estadístico observado, podemos construir una distribución para la hipótesis nula. Recordando el Teorema del Límite Central (TLC), sabemos que, si tomamos muchas muestras de una población y calculamos su estadístico (como la media), los resultados tienden a seguir una distribución aproximadamente normal alrededor del valor supuesto por la hipótesis nula. Esta distribución para la hipótesis nula nos indica qué tan probable es obtener diferentes valores del estadístico bajo la suposición de que no hay efecto o diferencia real.\n\n\n\n\n\n\n\n\n\nIII. Nivel de confianza y definición de la zonas criticas:\nLa esencia de un contraste de hipótesis radica en evaluar qué tan diferente o extremo debe ser un valor observado para poder afirmar que no es probable si la hipótesis nula (\\(H_0\\)) fuera cierta, incluso considerando la variabilidad natural que ocurre por azar. En otras palabras, el contraste de hipótesis busca determinar si el resultado obtenido se desvía tanto de lo esperado bajo \\(H_0\\) que podemos justificar su rechazo. Este proceso nos permite distinguir entre variaciones que pueden explicarse simplemente por la aleatoriedad inherente al muestreo y aquellas que son lo suficientemente inusuales como para sugerir que existe un efecto real o una diferencia significativa.\nPara evaluar si un resultado es lo suficientemente extremo como para considerarlo evidencia en contra de \\(H_0\\), necesitamos definir un umbral que nos ayude a tomar esta decisión. Este umbral se establece a través del nivel de confianza y el nivel de significancia (\\(\\alpha\\)).\nSabiendo que:\n\\[ \\text{Nivel de confianza} = 1 - \\alpha \\]\nEl nivel de significancia \\(\\alpha\\) nos dice cuánto estamos dispuestos a aceptar el riesgo de cometer un Error Tipo I (rechazar \\(H_0\\) cuando en realidad es cierta). Si elegimos \\(\\alpha = 0.05\\), estamos definiendo que solo el 5% de los valores más extremos de la distribución nula serán considerados “demasiado improbables” bajo \\(H_0\\).\nPor ejemplo, si elegimos un nivel de confianza del 95%, significa que queremos estar seguros en un 95% de que la conclusión que tomemos será correcta y aceptamos un 5% de riesgo de cometer un Error Tipo I (rechazar \\(H_0\\) cuando en realidad es cierta). Esto implica que el nivel de significancia en este caso es:\n\\[ \\alpha = 0.05 \\]\nEl valor de \\(\\alpha\\) nos ayuda a definir las zonas críticas de la distribución nula, que son las regiones donde un resultado se considerará lo suficientemente improbable como para rechazar \\(H_0\\). La ubicación exacta de estas zonas depende del tipo de prueba que estemos realizando\nSi realizamos una prueba bilateral (de dos colas), el nivel de significancia se divide en dos extremos de la distribución nula, dejando \\(\\frac{\\alpha}{2}\\) en cada cola. Esto significa que rechazamos \\(H_0\\) si el valor observado es demasiado grande o demasiado pequeño en comparación con lo que esperaríamos si \\(H_0\\) fuera cierta.\n\n\n\n\n\n\n\n\n\nSi realizamos una prueba unilateral (de una cola), todo el nivel de significancia \\(\\alpha\\) se coloca en una sola cola de la distribución, dependiendo de si estamos evaluando un efecto mayor o menor. En este caso, rechazamos \\(H_0\\) solo si el resultado observado es significativamente mayor o significativamente menor de lo esperado bajo \\(H_0\\).\n\n\n\n\n\n\n\n\n\nPara determinar qué tan extremo debe ser un resultado para considerarlo estadísticamente significativo, usamos el valor crítico (\\(z\\) crítico o \\(t\\) crítico) que determina la distancia en errores estándar de la media de la distribución.\nPor ejemplo, en una prueba basada en la distribución normal estándar, el \\(z\\) crítico para \\(\\alpha = 0.05\\) en una prueba bilateral es el 2.5% más extremo a cada lado, por lo que aproximadamente \\(\\pm1.96\\)\n\nqnorm(0.975)\n\n[1] 1.96\n\nqnorm(0.025)\n\n[1] -1.96\n\n\n\n\n\n\n\n\n\n\n\nEsto que significa que cualquier estadístico que caiga fuera de este rango se considera lo suficientemente improbable bajo \\(H_0\\) como para rechazarla. Además, si la muestra es pequeña o la varianza es desconocida, usamos la distribución \\(t\\) de Student, en cuyo caso el \\(t\\) crítico dependerá del tamaño muestral y los grados de libertad.\nIV. Comparación del Valor Observado con la Distribución Nula\nYa que escogimos \\(\\alpha = 0.05\\), si el valor observado cae en el 5% más extremo de la distribución nula (es decir, en las áreas menos probables), lo consideramos lo suficientemente inusual como para que no pueda explicarse únicamente por la variabilidad aleatoria bajo la hipótesis nula (\\(H_0\\)). En este caso, rechazamos \\(H_0\\) y favorecemos la hipótesis alternativa (\\(H_1\\)).\nSi el valor observado está dentro del 95% de la distribución nula, significa que se encuentra dentro del rango esperado por variación natural. No es lo suficientemente inusual, por lo que no rechazamos \\(H_0\\). En cambio, si el valor observado cae en el 5% más extremo de la distribución, consideramos que es lo suficientemente improbable bajo \\(H_0\\), por lo que tenemos suficiente evidencia para rechazarla en favor de \\(H_1\\).\nUtilizamos el valor \\(t\\) y el valor \\(z\\) para evaluar cuán lejos está un valor observado de lo que esperaríamos bajo la hipótesis nula.\n\nPara pruebas sobre una media: \\[\nt = \\frac{\\bar{x} - \\mu_0}{\\text{Error estándar}}, \\quad \\text{donde } \\mu_0 \\text{ es la media bajo } H_0.\n\\]\nPara pruebas sobre una proporción: \\[\nz = \\frac{\\hat{p} - p_0}{\\text{Error estándar}}, \\quad \\text{donde } p_0 \\text{ es la proporción bajo } H_0.\n\\] No olvidar que los valores \\(z\\) y \\(t\\) representan distancia (en errores estándar) con respecto al centro de la distribución.\n\nV. Decisión Basada en el Nivel de Significancia (\\(\\alpha\\)) y el p-valor:\nEl contraste de hipótesis, en esencia, usa una distribución nula para comparar los resultados observados con lo que esperaríamos bajo la hipótesis nula. Esto permite hacer una evaluación objetiva, decidiendo si el resultado es suficientemente extremo como para ser significativo o si podría explicarse simplemente por la variabilidad natural en los datos. Por lo que utilizamos nuestro estadístico observado y evaluamos si se encuentra en la zona crítica expresada en valores \\(t\\) o \\(z\\).\n\\[\n  \\text{Región de rechazo: } |t| &gt; t_{\\text{crítico}} \\quad \\text{o} \\quad |z| &gt; z_{\\text{crítico}}\n\\]\nEl valor \\(t\\) o \\(z\\) observado puede estar tanto a la izquierda como a la derecha, por ello su valor absoluto.\nHay que ser cautelosos ya que con este proceso es que no estamos diciendo que \\(H_0\\) sea absolutamente falsa, sino que el resultado observado en nuestra muestra es lo suficientemente improbable como para justificar dudar de ella y considerar que \\(H_1\\) es más plausible.\nOtra forma de evaluar si el resultado es suficientemente extremo es mediante el p-valor, que representa la probabilidad de obtener un estadístico de prueba igual o más extremo que el observado, bajo la suposición de que \\(H_0\\) es verdadera.\nAmbos enfoques (comparar el estadístico con el valor crítico o utilizar el p-valor) son dos caras de la misma moneda y llevan a la misma conclusión. Mientras que la comparación con el valor crítico permite establecer de manera directa si un resultado cae dentro de la región de rechazo, el p-valor ofrece una medida de cuán improbable es el resultado bajo \\(H_0\\).\nPara tomar una decisión utilizando el p-valor:\n\nSi \\(p \\leq \\alpha\\) → Se rechaza \\(H_0\\) ya que el resultado es demasiado improbable bajo la hipótesis nula.\n\nSi \\(p &gt; \\alpha\\) → No se rechaza \\(H_0\\) ya que el resultado no es lo suficientemente extremo como para concluir que hay una diferencia significativa.\n\nEste enfoque es especialmente útil porque las pruebas estadísticas en R reportan automáticamente el p-valor.\nRecuerda que tenemos una hipótesis nula (\\(H_0\\)) que representa la idea de que no hay efecto, no hay diferencia, o que se debe al azar. Por ejemplo, si comparamos dos tratamientos, la hipótesis nula podría ser que ambos tienen el mismo efecto.\nEl p-valor es una forma de medir cuán compatibles son nuestros datos con esa hipótesis nula. Técnicamente, el p-valor es la probabilidad de obtener un resultado igual o más extremo que el observado, asumiendo que la hipótesis nula es verdadera.\n\n\n\n\n\n\nCuidado\n\n\n\nel p-valor no te dice la probabilidad de que la hipótesis nula sea cierta. Más bien, te dice qué tan raros serían tus datos si la hipótesis nula fuera cierta. Esta decisión no prueba que \\(H_0\\) sea verdadera o falsa, solo indica si hay evidencia suficiente, desde el punto de vista estadístico, para cuestionarla."
  },
  {
    "objectID": "sesion4.html#prueba-t-para-comparación-de-medias-entre-dos-grupos",
    "href": "sesion4.html#prueba-t-para-comparación-de-medias-entre-dos-grupos",
    "title": "Sesión 4: Probabilidad, simulación, pruebas estadísticas y modelos de regresión",
    "section": "Prueba t para comparación de medias entre dos grupos",
    "text": "Prueba t para comparación de medias entre dos grupos\nEsta prueba se usa cuando queremos saber si la media de un grupo difiere significativamente de la de otro grupo. Es especialmente útil en contextos de intervención o comparación.\nEjemplo: Un programa de capacitación fue implementado en algunas municipalidades. Se evalúa si el ingreso promedio mensual cambió respecto a un grupo sin capacitación.\n\n\\(H_0\\): la media del grupo tratado es igual a la del grupo control.\\\n\\(H_1\\): las medias son distintas (o, en una prueba unilateral, que una es mayor que la otra).\n\nEl estadístico t se calcula como:\n\\[\nt = \\frac{\\bar{x}_1 - \\bar{x}_2}{\\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}}\n\\]\nDonde \\(\\bar{x}_1\\) y \\(\\bar{x}_2\\) son las medias de cada grupo, \\(s_1^2\\) y \\(s_2^2\\) las varianzas, y \\(n_1\\), \\(n_2\\) los tamaños muestrales.\nEjemplo:estamos investigando si existen diferencias en el tiempo de ocio semanal entre hombres y mujeres. Para ello, ha recolectado información sobre la cantidad de horas que cada persona dedica al ocio en una semana y desea comparar las medias de ambos grupos.\n\nocio = read_csv('horas_ocio.csv')\n\n\nglimpse(ocio)\n\nRows: 200\nColumns: 2\n$ genero     &lt;chr&gt; \"hombre\", \"mujer\", \"hombre\", \"hombre\", \"mujer\", \"mujer\", \"m…\n$ horas_ocio &lt;dbl&gt; 15, 9, 13, 12, 13, 11, 13, 15, 14, 11, 14, 16, 10, 13, 13, …\n\n\nPodemos utilizar describeBy del paquete psych para realizar los resúmenes numéricos pertinentes por cada nivel (categoría) de la variable genero.\n\nlibrary(psych)\n\n\ndescribeBy(ocio$horas_ocio, ocio$genero)\n\n\n Descriptive statistics by group \ngroup: hombre\n   vars   n mean   sd median trimmed  mad min max range skew kurtosis   se\nX1    1 100 15.7 2.47     15    15.6 2.97  11  24    13 0.58     0.35 0.25\n------------------------------------------------------------ \ngroup: mujer\n   vars   n mean   sd median trimmed  mad min max range skew kurtosis   se\nX1    1 100 12.1 2.31     12    12.1 1.48   6  17    11    0    -0.39 0.23\n\n\nEsto nos permite comparar cómo varían las horas de ocio entre ambos grupos, pero una manera aún más intuitiva de explorar estas diferencias es mediante una representación gráfica. Para ello, utilizamos un boxplot, que nos muestra la distribución del tiempo de ocio según el género.\n\nocio %&gt;% \n  ggplot(aes(x = genero, y = horas_ocio, fill = genero)) +\n  geom_boxplot() +\n  labs(title = 'Horas de ocio semanal por género', \n       y = 'Cantidad de horas') +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nEl gráfico muestra una diferencia en la distribución entre los grupos. La clave aquí es que tan diferentes son como para poder afirmar que efectivamente el género influye o explica la variación en las horas de ocio.\nPodemos generar un resumen descriptivo con la media y la desviación estándar utilizando dplyr\n\nestadisticas = ocio %&gt;%\n  group_by(genero) %&gt;%\n  summarise(\n    promedio = mean(horas_ocio),\n    sd = sd(horas_ocio),\n    n = n()\n  )\n\nestadisticas\n\n\n\n\n# A tibble: 2 × 4\n  genero promedio    sd     n\n  &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n1 hombre     15.7  2.47   100\n2 mujer      12.1  2.31   100\n\n\n\nPara cuantificar esta diferencia, necesitamos calcular un estadístico de comparación, que en este caso es la diferencia de medias muestrales, representada por \\(\\Delta\\) (Delta).\n\ndiferencia = diff(estadisticas$promedio)\ndiferencia\n\n[1] -3.6\n\n\nPor lo que la diferencia de medias \\(\\Delta \\bar{x}\\) es: \\[\n\\Delta \\bar{x} = \\bar{x}_{Hombres} - \\bar{x}_{Mujeres} = 15.8 - 12.2 = 3.6\n\\]\n\n\n\n\n\n\n\n\n\nPlanteamiento de Hipótesis\nEl primer paso es formular nuestras hipótesis estadísticas:\n\nHipótesis nula (\\(H_0\\)): No existe diferencia entre las medias de las dos poblaciones. En otras palabras: \\[\nH_0: \\mu_{Hombres} - \\mu_{Mujeres} = 0\n\\]\nHipótesis alternativa (\\(H_1\\)): Existe una diferencia significativa entre las medias de las dos poblaciones: \\[\nH_1: \\mu_{Hombres} - \\mu_{Mujeres} \\neq 0\n\\]\n\nEl contraste de hipótesis nos permite evaluar si la diferencia observada en las horas de ocio promedio es lo suficientemente grande como para no atribuirse al azar. Bajo la hipótesis nula (\\(H_0\\)), asumimos que el género no influye en las horas de ocio, es decir, que la diferencia de medias en la población es cero (\\(\\mu_1 = \\mu_2\\)). Si rechazamos (\\(H_0\\)), concluimos que el género sí tiene un efecto significativo.\nEn nuestro caso, hemos planteado una prueba bilateral, donde la hipótesis alternativa (\\(H_A\\)) simplemente indica que existe una diferencia entre los grupos, sin especificar la dirección:\n\\[\nH_A: \\mu_1 \\neq \\mu_2\n\\]\n\nlibrary(broom)\n\n\nprueba_t = t.test(horas_ocio ~ genero, \n                  data = ocio, \n                  conf.level = 0.95)\n\ntidy(prueba_t)\n\n# A tibble: 1 × 10\n  estimate estimate1 estimate2 statistic  p.value parameter conf.low conf.high\n     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1      3.6      15.7      12.1      10.7 3.04e-21      197.     2.93      4.27\n# ℹ 2 more variables: method &lt;chr&gt;, alternative &lt;chr&gt;\n\n\n\nInterpretación:\n\nSi \\(t\\) es muy alto en valor absoluto (es decir, muy alejado del 0), y el p-valor &lt; \\(\\alpha\\) rechazamos \\(H_0\\).\nEsto indica que la diferencia de medias es suficientemente grande como para no atribuirse solo al azar.\n\n\n\n\n\n\n\nImportante\n\n\n\nSi las varianzas no son iguales, se usa la versión de Welch. La prueba asume normalidad o un tamaño muestral grande (por el TLC)."
  },
  {
    "objectID": "sesion4.html#test-de-proporciones",
    "href": "sesion4.html#test-de-proporciones",
    "title": "Sesión 4: Probabilidad, simulación, pruebas estadísticas y modelos de regresión",
    "section": "Test de proporciones",
    "text": "Test de proporciones\nEl contraste de hipótesis para la diferencia de proporciones entre dos grupos sigue una estructura similar al de medias, pero evalúa si la diferencia observada entre las proporciones muestrales es suficientemente extrema como para concluir que no se debe al azar.\nEjemplo: Queremos saber si el 30% de apoyo a una reforma educativa en una encuesta nacional difiere del 50% que se espera históricamente.\n\n\\(H_0\\): \\(p = 0.5\\)\n\\(H_1\\): \\(p \\neq 0.5\\) (prueba bilateral)\n\nEl estadístico es:\n\\[\nz = \\frac{\\hat{p} - p_0}{\\sqrt{\\frac{p_0(1 - p_0)}{n}}}\n\\]\nDonde:\n\n\\(\\hat{p}\\) es la proporción observada,\n\\(p_0\\) es la proporción bajo \\(H_0\\),\n\\(n\\) es el tamaño muestral.\n\nRegión 1: 80 de 100 personas aprueban Región 2: 70 de 100 personas aprueban\n\nprop.test(x = c(80, 70), \n          n = c(100, 100))\n\n\n    2-sample test for equality of proportions with continuity correction\n\ndata:  c(80, 70) out of c(100, 100)\nX-squared = 2, df = 1, p-value = 0.1\nalternative hypothesis: two.sided\n95 percent confidence interval:\n -0.0292  0.2292\nsample estimates:\nprop 1 prop 2 \n   0.8    0.7 \n\n\nInterpretación:\n\nSi el valor \\(z\\) cae fuera del intervalo crítico, o si el p-valor es menor a \\(\\alpha\\), rechazamos \\(H_0\\).\n\nTambién puede aplicarse para comparar dos proporciones:\n\\[\nz = \\frac{\\hat{p}_1 - \\hat{p}_2}{\\sqrt{\\hat{p}(1 - \\hat{p}) \\left( \\frac{1}{n_1} + \\frac{1}{n_2} \\right)}}\n\\]\nDonde \\(\\hat{p}\\) es la proporción combinada."
  },
  {
    "objectID": "sesion4.html#prueba-chi-cuadrado-para-asociación-entre-variables-categóricas",
    "href": "sesion4.html#prueba-chi-cuadrado-para-asociación-entre-variables-categóricas",
    "title": "Sesión 4: Probabilidad, simulación, pruebas estadísticas y modelos de regresión",
    "section": "Prueba chi-cuadrado para asociación entre variables categóricas",
    "text": "Prueba chi-cuadrado para asociación entre variables categóricas\nLa prueba chi-cuadrado evalúa si dos variables categóricas están asociadas o si son independientes.\nEjemplo: Queremos saber si la percepción de inseguridad (alta vs. baja) está asociada al sexo (hombre vs. mujer).\n\n\\(H_0\\): las variables son independientes.\n\\(H_1\\): hay asociación entre las variables.\n\nConstruimos una tabla de contingencia con frecuencias observadas, y se compara con las frecuencias esperadas bajo la hipótesis de independencia.\nEl estadístico se calcula así:\n\\[\n\\chi^2 = \\sum \\frac{(O_{ij} - E_{ij})^2}{E_{ij}}\n\\]\nDonde \\(O_{ij}\\) son las frecuencias observadas, y \\(E_{ij}\\) las esperadas.\nCreamos una tabla de contingencia\n\nset.seed(123)\n\ntabla = matrix(c(60, 40, 30, 70), nrow = 2,\n                dimnames = list(\"Educación\" = c(\"Universitario\", \n                                                \"No universitario\"),\n                                \"Empleo\" = c(\"Formal\", \n                                             \"Informal\")))\n\n\n\ntabla\n\n                  Empleo\nEducación         Formal Informal\n  Universitario        60       30\n  No universitario     40       70\n\n\n\nPrueba chi-cuadrado\n\nchisq.test(tabla)\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  tabla\nX-squared = 17, df = 1, p-value = 0.00004\n\n\nInterpretación:\n\nUn valor alto de \\(\\chi^2\\) indica que las diferencias entre observados y esperados son grandes → evidencia contra \\(H_0\\).\nSi el p-valor asociado es bajo, rechazamos \\(H_0\\) → hay evidencia de asociación.\n\nPrecauciones:\n\nLas frecuencias esperadas no deben ser demasiado pequeñas (al menos 5 idealmente en cada celda).\nNo mide fuerza de asociación, solo existencia.\n\n\n\n\n\n\n\nImportante\n\n\n\nLas frecuencias esperadas no deben ser demasiado pequeñas (al menos 5 idealmente en cada celda). No mide fuerza de asociación, solo existencia."
  },
  {
    "objectID": "sesion4.html#en-conclusión",
    "href": "sesion4.html#en-conclusión",
    "title": "Sesión 4: Probabilidad, simulación, pruebas estadísticas y modelos de regresión",
    "section": "En conclusión",
    "text": "En conclusión\nTodas estas pruebas siguen la misma lógica inferencial, que ya vimos con el Teorema Central del Límite y el valor p:\n\nSi el estadístico de prueba cae en la zona crítica o el p-valor es pequeño, el resultado observado es demasiado raro bajo \\(H_0\\), por lo que la rechazamos.\nCuanto mayor el tamaño de muestra, menor el error estándar, y más precisa será nuestra decisión."
  },
  {
    "objectID": "sesion4.html#anova",
    "href": "sesion4.html#anova",
    "title": "Sesión 4: Probabilidad, simulación, pruebas estadísticas y modelos de regresión",
    "section": "ANOVA",
    "text": "ANOVA\nCuando comparamos variación en las medias de los dinstitnos grupos, la prueba de ANOVA (Análisis de Varianza) es la herramienta adecuada. Las pruebas ANOVA siguen la misma lógica e untición matemática vista en las anteriores pruebas pero los cálculos matemáticos son algo más complejas y están fuera del propósito del libro, aunque te animo a revisarlas por ti mismo.\nImagina que estamos investigando si existe una diferencia en la satisfacción laboral entre distintos sectores económicos. Para ello, recopilamos datos de empleados en cinco sectores: Tecnología, Finanzas, Educación, Salud y Manufactura. Su objetivo es determinar si el nivel de satisfacción laboral varía significativamente entre los grupos.\nPara medir la satisfacción, cada empleado calificó su experiencia laboral en una escala de 0 a 10, donde:\n\n\n\nPuntuación\nNivel de Satisfacción\n\n\n\n\n0 - 2\nMuy insatisfecho\n\n\n3 - 4\nInsatisfecho\n\n\n5 - 6\nNeutral\n\n\n7 - 8\nSatisfecho\n\n\n9 - 10\nMuy satisfecho\n\n\n\nANOVA nos permite comparar las medias de tres o más grupos y determinar si al menos uno de ellos es significativamente diferente de los demás. En lugar de realizar múltiples t-tests (lo que aumentaría el riesgo de error tipo I), ANOVA evalúa si la variabilidad entre los grupos es mayor que la variabilidad dentro de cada grupo, lo que indicaría que las diferencias observadas no se deben al azar.\nEn pruebas anteriores vimos que, cuando comparamos dos grupos, usamos la distribución t para evaluar la diferencia entre sus medias. La distribución \\(t\\) es útil para comparar dos muestras porque se basa en la diferencia de medias y la variabilidad dentro de cada grupo. En lugar de eso, ANOVA utiliza la distribución F, que compara la variabilidad entre los grupos con la variabilidad dentro de los grupos en una sola prueba.\nEl estadístico F se define como:\n\\[\nF = \\frac{\\text{Variabilidad entre grupos}}{\\text{Variabilidad dentro de los grupos}}\n\\]\n\n\n¿Pero por qué nos interesa comparar la variabilidad entre los grupos con la variabilidad dentro de los grupos? Porque esto nos dice si las diferencias observadas son reales o simplemente producto del azar. Si la variabilidad entre los grupos es similar a la variabilidad dentro de ellos, significa que las diferencias en las medias no son mayores que las fluctuaciones normales que ocurren dentro de cada grupo. En otras palabras, aunque haya cierta dispersión en los datos, esta dispersión es comparable en todos los grupos, por lo que no hay razones para pensar que alguno de ellos sea realmente diferente.\n\nSin embargo, si la variabilidad entre los grupos es mucho mayor que la variabilidad dentro de ellos, entonces estamos viendo diferencias que van más allá de la variación esperada dentro de cada grupo. Esto nos sugiere que al menos un grupo tiene una media que se aleja significativamente de los demás. Esa es la clave del ANOVA.\nLa distribución F nos permite determinar qué tan extremo debe ser un valor F para rechazar la hipótesis nula. Valores bajos de F indican que las diferencias entre los grupos no son mayores que las diferencias dentro de ellos, mientras que valores altos sugieren que al menos un grupo es significativamente distinto. Por eso, en las pruebas ANOVA, cuando obtenemos un F suficientemente grande, podemos concluir que hay diferencias estadísticamente significativas entre los grupos analizados.\nEl primer paso es formular nuestras hipótesis estadísticas:\n\nHipótesis nula (\\(H_0\\)): No existe diferencia en la satisfacción laboral entre los sectores. Es decir:\n\n\\[\n    H_0: \\mu_1 = \\mu_2 = \\mu_3 = \\mu_4 = \\mu_5\n\\]\n\nHipótesis alternativa (\\(H_1\\)): Al menos un sector tiene una media de satisfacción diferente:\n\n\\[\n    H_1: \\text{Al menos un } \\mu_i \\text{ es diferente}\n\\]\nBajo (\\(H_0\\)), los niveles de satisfacción en los distintos sectores son iguales. Si rechazamos (\\(H_0\\)), concluimos que existe una diferencia significativa en la satisfacción laboral entre al menos un grupo.\nCargamos los datos de la encuesta\n\nsatisfaccion = read_csv('base_satisfaccion.csv')\n\nAntes de realizar la prueba ANOVA, exploramos las características de cada grupo:\n\n# Estadísticas descriptivas por sector\nsatisfaccion %&gt;%\n  group_by(sector) %&gt;%\n  summarise(\n    media = mean(nivel_satisfaccion),\n    desviacion = sd(nivel_satisfaccion),\n    n = n()\n  )\n\n\n\n\n# A tibble: 5 × 4\n  sector      media desviacion     n\n  &lt;chr&gt;       &lt;dbl&gt;      &lt;dbl&gt; &lt;int&gt;\n1 Educación    4.9        2.89    30\n2 Finanzas     4.03       2.74    30\n3 Manufactura  2.4        1.40    30\n4 Salud        5.97       2.46    30\n5 Tecnología   4.93       2.50    30\n\n\nPara visualizar las diferencias en satisfacción laboral entre sectores, podemos utilizar un boxplot:\n\nggplot(satisfaccion, aes(x = sector, \n                         y = nivel_satisfaccion, \n                         fill = sector)) +\n  geom_boxplot() +\n  labs(title = \"Distribución de la satisfacción laboral por sector\",\n       x = \"Sector Económico\",\n       y = \"Nivel de Satisfacción\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nUtilizamos la función aov() que realiza un ANOVA. Esta función ajusta un modelo estadístico donde la variable repsuesta (satisfacción laboral) se compara entre los niveles de la variable explicativa (sector económico). Al ejecutar summary(), obtenemos una tabla con los resultados del análisis, incluyendo el estadístico F y su p-valor, que nos indican si existe una diferencia significativa entre los grupos. Fíjate en el Pr(&gt;F)\nRealizar ANOVA de una vía para comparar la satisfacción laboral entre sectores\n\nanova_resultado = aov(nivel_satisfaccion ~ sector, \n                      data = satisfaccion)\n\n\nMostrar el resumen del ANOVA con el estadístico F y el p-valor\n\nsummary(anova_resultado)\n\n             Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nsector        4    213    53.3    8.85 0.000002 ***\nResiduals   145    874     6.0                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "sesion4.html#regresión-lineal-simple-una-sola-variable-predictora",
    "href": "sesion4.html#regresión-lineal-simple-una-sola-variable-predictora",
    "title": "Sesión 4: Probabilidad, simulación, pruebas estadísticas y modelos de regresión",
    "section": "Regresión lineal simple (una sola variable predictora)",
    "text": "Regresión lineal simple (una sola variable predictora)\nEjemplo: Tenemos un conjunto de datos sobre precios de viviendas. Este conjunto contiene información sobre el precio de viviendas en Taiwan y su relación con otras variables como la distancia a la estación de METRO más cercana (dist_to_mrt_m, medida en metros) y el precio promedio por metro cuadrado (price_twd_msq, medido en dólares taiwaneses).\nPodemos utilizar un gráfico de dispersión para explorar cómo el precio de las viviendas podría variar según la proximidad al transporte público.\n\ncasas = read_csv('taiwan_casas.csv')\n\n\nglimpse(casas)\n\nRows: 414\nColumns: 4\n$ dist_to_mrt_m   &lt;dbl&gt; 84.9, 306.6, 562.0, 562.0, 390.6, 2175.0, 623.5, 287.6…\n$ n_convenience   &lt;dbl&gt; 10, 9, 5, 5, 5, 3, 7, 6, 1, 3, 1, 9, 5, 4, 4, 2, 6, 1,…\n$ house_age_years &lt;chr&gt; \"30 to 45\", \"15 to 30\", \"0 to 15\", \"0 to 15\", \"0 to 15…\n$ price_twd_msq   &lt;dbl&gt; 11.47, 12.77, 14.31, 16.58, 13.04, 9.71, 12.19, 14.13,…\n\n\nUtilizamos geom_point. Dado que hay una gran superposición entre los puntos, es recomendable ajustar el parámetro alpha para controlar la transparencia y así mejorar la visualización, permitiendo distinguir mejor la densidad de los datos.\n\ng_casa = casas %&gt;%\n  ggplot(aes(x = dist_to_mrt_m, y = price_twd_msq)) +\n  geom_point(alpha = 0.6, # Útil para distinguir mejor la aglomeración\n             color = \"darkblue\") +  # Puntos semitransparentes en azul\n  labs(\n    title = \"Relación entre distancia al Metro y precio por metro cuadrado\",\n    x = \"Distancia al metro (metros)\",\n    y = \"Precio (TWD/m²)\"\n  ) +\n  theme_minimal()  \n\ng_casa\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPodemos notar un tendencia negativa conforma la distancia de la vivienda es mayor a una estación de metro. Si nuestro objetivo es analizar cómo los valores de la variable respuesta (\\(y\\)) cambian en función de la variable explicativa (\\(x\\)). Al observar los gráficos de dispersión, podemos identificar un patrón: parece que hay una relación entre las variables, y esta relación muestra un comportamiento que, a simple vista, podríamos aproximar como lineal. Basándonos en esta observación, proponemos que la relación entre \\(x\\) e \\(y\\) sea lineal, es decir, que los cambios en la variable explicativa (\\(x\\)) produzcan cambios proporcionales (constantes) en la variable respuesta (\\(y\\)).\n\nEcuación de regresión lineal\nDe hecho, podemos representar la linea recta que mejor represente esta relación entre las variables utilizando geom_smooth y definiendo un modelo lineal o linear model (lm).\n\ng_casa +\n  geom_smooth(method = 'lm',\n              color = 'red',\n              se = F)\n\n\n\n\n\n\n\n\n\n\n\nEsta línea no es solo una herramienta visual, también nos permite aproximar cómo los cambios en una variable (la explicativa, \\(x\\)) afectan a otra variable (la respuesta, \\(y\\)) de forma intuitiva. Siendo una línea recta, podemos describirla con una fórmula matemática. La ecuación de la regresión lineal es la siguiente:\n\\[y = \\alpha + \\beta X\\]\nDonde:\n\\[Y(\\text{variable respuesta}) = \\alpha + \\beta X(\\text{variable explicativa})\\]\n\nEn R podemos ajustar el modelo de regresión lineal y extraer los parametros de ecuación usando lm(). Su estructura básica sigue la lógica de una fórmula: lm(y ~ x, data = ...), donde y representa la variable dependiente o respuesta, y x la variable independiente. El símbolo ~ se lee como “explicado por” o “en función de”, por lo que la expresión y ~ x indica que estamos tratando de explicar los valores de y a partir de los valores de x. El argumento data = ... especifica el nombre del data frame donde se encuentran esas variables.\nAjustamos el modelo lineal\n\nmodelo = lm(price_twd_msq ~ dist_to_mrt_m, data = casas)\n\nVemos los coeficientes (intercepto y pendiente)\n\ncoef(modelo)\n\n  (Intercept) dist_to_mrt_m \n      13.8734       -0.0022 \n\n\n\nCon esta información, ya podemos escribir la ecuación del modelo muestral:\n\\[\n\\text{Precio de la vivienda} = 13.87 - 0.0022 \\cdot \\text{Distancia en }m^2\n\\]\nEsto significa que, en base a la información que tenemos disponible sobre las viviendas, la pendiente del modelo es \\(-0.0022\\). Es decir, por cada metro adicional de distancia a la estación de metro, el precio promedio por metro cuadrado disminuye en alrededor de 0.0022 dólares taiwaneses. Por otro lado, el intercepto es \\(13.87\\), lo que indica que cuando la distancia a la estación es cero (es decir, en una vivienda ubicada justo al lado del metro, \\(x=0\\)) el precio estimado por metro cuadrado sería de 13.87 dólares taiwaneses. Si bien en la práctica puede que no existan viviendas exactamente a cero metros del metro, este valor nos da una referencia inicial del modelo y completa la ecuación de la línea recta ajustada.\n\nAunque esta representación matemática puede parecer un poco abstracta al principio, es simplemente una manera formal de expresar algo que podemos ver intuitivamente en el gráfico. La línea recta nos ayuda a simplificar y entender mejor los patrones en nuestros datos. Sin embargo, como toda simplificación, debemos ser conscientes de sus limitaciones: no todos los puntos se alinean perfectamente con la línea, por lo que la relación que estamos estableciendo es una aproximación. Todos los modelos son aproximaciones simples de la realidad. Un modelo es, en esencia, una representación simplificada de la relación entre variables en una población. Nos ayuda a interpretar patrones observados en los datos.\n\nSin embargo, es fundamental destacar que este tipo de modelos no pretende capturar toda la complejidad de la realidad. Más bien, funcionan como una herramienta para aproximarnos a las relaciones observadas en los datos. Es crucial recordar que correlación no implica causalidad, y que la relación entre variables dentro de un modelo estadístico no siempre refleja un vínculo de causa y efecto.\n\n\n\nResumen del modelo en R con summary()\nEn R podemos ajustar el modelo con lm(), y una vez que le asignamos un nombre, podemos usar summary() para obtener un resumen completo que incluye todos los elementos que hemos comentado hasta el momento y algunos más.\n\nsummary(modelo)\n\n\nCall:\nlm(formula = price_twd_msq ~ dist_to_mrt_m, data = casas)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-10.710  -1.818  -0.362   1.462  22.234 \n\nCoefficients:\n               Estimate Std. Error t value            Pr(&gt;|t|)    \n(Intercept)   13.873352   0.197462    70.3 &lt;0.0000000000000002 ***\ndist_to_mrt_m -0.002197   0.000119   -18.5 &lt;0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.05 on 412 degrees of freedom\nMultiple R-squared:  0.454, Adjusted R-squared:  0.452 \nF-statistic:  342 on 1 and 412 DF,  p-value: &lt;0.0000000000000002"
  },
  {
    "objectID": "sesion4.html#regresión-múltiple",
    "href": "sesion4.html#regresión-múltiple",
    "title": "Sesión 4: Probabilidad, simulación, pruebas estadísticas y modelos de regresión",
    "section": "Regresión múltiple",
    "text": "Regresión múltiple\nComo vimos, la regresión lineal simple busca ajustar un modelo lineal que describa la relación entre una variable respuesta cuantitativa y una sola variable explicativa. Sin embargo, en muchos casos, especialmente en ciencias sociales, esta relación no puede entenderse de forma aislada. La realidad suele estar influida por múltiples factores al mismo tiempo, por lo que se vuelve necesario ampliar el enfoque e incorporar más de una variable explicativa. Esto puede responder a razones teóricas, cuando distintas variables son relevantes para explicar un fenómeno, o a motivaciones analíticas, como explorar mejor la estructura de los datos o evitar interpretaciones sesgadas por omisión de variables importantes.\n\nCuando establecemos un modelo de regresión con más de una variable explicativa, hablamos de una regresión lineal múltiple. En este tipo de modelos seguimos asumiendo que existe una relación lineal, es decir, de variación proporcional, entre cada variable explicativa y la respuesta \\(y\\), aunque ahora la línea se transforma en un plano o hiperplano, dependiendo del número de predictores. A diferencia del modelo simple, en la regresión múltiple ya no buscamos entender el efecto de una sola variable sobre \\(y\\), sino cómo varias variables, en conjunto, contribuyen a explicar su comportamiento.\n\nLa representación gráfica del modelo múltiple no es tan directa. Más allá de tres variables, no podemos visualizar de forma intuitiva el espacio geométrico del modelo, por lo que el análisis visual se vuelve más abstracto y la atención se centra en la interpretación de sus coeficientes.\n\n\n\n\n\n\n\n\n\n\n\nModelo de regresión múltiple\nY entonces, ¿qué hay de la ecuación del modelo? Si ya entendimos cómo funciona la regresión lineal simple, ahora toca ver cómo se generaliza cuando agregamos más de una variable explicativa.\n\nEn la regresión simple trabajamos con dos parámetros clave: el intercepto (\\(\\alpha\\)) y la pendiente (\\(\\beta\\)), donde \\(\\beta\\) representaba el cambio promedio en la variable respuesta \\(y\\) por cada unidad adicional en la variable explicativa \\(x\\). La regresión múltiple toma esta lógica básica y la extiende, permitiéndonos incluir varias variables explicativas a la vez. Desde el punto de vista metodológico, esto significa que ya no estamos observando una relación aislada entre dos variables, sino que queremos entender cómo un conjunto de predictores contribuyen en conjunto a explicar la variación en \\(y\\).\n\nEste modelo se expresa de la siguiente manera:\n\\[\ny_i = \\alpha + \\beta_1 x_{1i} + \\beta_2 x_{2i} + \\dots + \\beta_p x_{pi}\n\\]\n\nEn R, ajustar un modelo de regresión lineal múltiple es tan sencillo como en el caso simple. La función lm() se utiliza de la misma manera, pero ahora incluimos más de una variable explicativa en la fórmula. Por ejemplo, si quisieramos queremos modelar el precio por metro cuadrado (price_twd_msq) como una función de la distancia a la estación de metro (dist_to_mrt_m) y ahora también del número de tiendas de conveniencia cercanas (n_convenience). El código sería el siguiente:\n\n\nmodelo_multiple = lm(price_twd_msq ~ dist_to_mrt_m + n_convenience, \n                     data = casas)\n\n\n\nsummary(modelo_multiple)\n\n\n\n\n\nCall:\nlm(formula = price_twd_msq ~ dist_to_mrt_m + n_convenience, data = casas)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-11.048  -1.774  -0.411   1.447  23.779 \n\nCoefficients:\n               Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept)   11.837489   0.393194   30.11 &lt; 0.0000000000000002 ***\ndist_to_mrt_m -0.001688   0.000143  -11.80 &lt; 0.0000000000000002 ***\nn_convenience  0.362360   0.061291    5.91         0.0000000071 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.93 on 411 degrees of freedom\nMultiple R-squared:  0.497, Adjusted R-squared:  0.494 \nF-statistic:  203 on 2 and 411 DF,  p-value: &lt;0.0000000000000002"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "sesion2.html",
    "href": "sesion2.html",
    "title": "Sesión 2: Manipulación de datos, transformación de variables y resúmenes estadísticos",
    "section": "",
    "text": "Aprender a transformar, organizar y resumir datos utilizando las herramientas del paquete dplyr. Incorporar principios estadísticos básicos para la descripción de variables. Reconocer patrones, errores y estructuras dentro de los datos."
  },
  {
    "objectID": "sesion2.html#objetivos-de-la-sesión-de-hoy",
    "href": "sesion2.html#objetivos-de-la-sesión-de-hoy",
    "title": "Sesión 2: Manipulación de datos, transformación de variables y resúmenes estadísticos",
    "section": "",
    "text": "Aprender a transformar, organizar y resumir datos utilizando las herramientas del paquete dplyr. Incorporar principios estadísticos básicos para la descripción de variables. Reconocer patrones, errores y estructuras dentro de los datos."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Web_Datafora",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.\n\n1 + 1\n\n[1] 2"
  }
]